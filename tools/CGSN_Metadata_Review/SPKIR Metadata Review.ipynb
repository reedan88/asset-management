{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPKIR Metadata Review\n",
    "\n",
    "This notebook describes the process for reviewing the calibration coefficients for the SPKIR. The purpose is to check the calibration coefficients contained in the CSVs stored within the asset management repository on GitHub, which are the coefficients utilized by OOI-net for calculating data products, against the different available sources of calibration information to identify when errors were made during entering the calibration csvs. This includes checking the following information:\n",
    "1. The calibration date - this information is stored in the filename of the csv\n",
    "2. Calibration source - identifying all the possible sources of calibration information, and determine which file should supply the calibration info\n",
    "3. Calibration coeffs - checking the accuracy and precision of the numbers stored in the calibration coefficients\n",
    "\n",
    "The SPKIRs contains three different calibration coefficients to check. All three of the coefficients are arrays of seven values. The possible calibration sources for the SPKIR are vendor calibration (.cal) files. The QCT, pre- and post-deployment files do not contain the relevant calibration information needed to perform checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**====================================================================================================================**\n",
    "Define the directories where the QCT, Pre, and Post deployment document files are stored, where the vendor documents are stored, where asset tracking is located, and where the calibration csvs are located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_directory = '/media/andrew/OS/Users/areed/Documents/Project_Files/Records/Instrument_Records/SPKIR/SPKIR_Results/'\n",
    "cal_directory = '/media/andrew/OS/Users/areed/Documents/Project_Files/Records/Instrument_Records/SPKIR/SPKIR_Cal/'\n",
    "asset_management_directory = '/home/andrew/Documents/OOI-CGSN/ooi-integration/asset-management/calibration/SPKIRB/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_spreadsheet = '/media/andrew/OS/Users/areed/Documents/Project_Files/Documentation/System/System Notebook/WHOI_Asset_Tracking.xlsx'\n",
    "sheet_name = 'Sensors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPKIR = whoi_asset_tracking(spreadsheet=excel_spreadsheet,sheet_name=sheet_name,instrument_class='SPKIR',series='B')\n",
    "SPKIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**======================================================================================================================**\n",
    "Now, I want to load all the calibration csvs and group them by UID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uids = sorted( list( set(SPKIR['UID']) ) )\n",
    "uids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_dict = {}\n",
    "asset_management = os.listdir(asset_management_directory)\n",
    "for uid in uids:\n",
    "    files = [file for file in asset_management if uid in file]\n",
    "    csv_dict.update({uid: sorted(files)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_paths = {}\n",
    "for uid in sorted(csv_dict.keys()):\n",
    "    paths = []\n",
    "    for file in csv_dict.get(uid):\n",
    "        path = generate_file_path(asset_management_directory, file, ext=['.csv','.ext'])\n",
    "        paths.append(path)\n",
    "    csv_paths.update({uid: paths})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=======================================================================================================================**\n",
    "The SPKIR QCT capture files are stored with the following Document Control Numbers (DCNs): 3305-00114-XXXXX. Most are storead as **.txt** or **.log** files. The problem is that the encoding of the data is not clear how the QCT is stored. Consequently, the QCT files aren't going to be used to check the SPKIR instrument calibration (for now).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qct_dict = get_qct_files(SPKIR, doc_directory)\n",
    "qct_paths = {}\n",
    "for uid in sorted(qct_dict.keys()):\n",
    "    paths = []\n",
    "    for file in qct_dict.get(uid):\n",
    "        path = generate_file_path(doc_directory, file)\n",
    "        paths.append(path)\n",
    "    qct_paths.update({uid: paths})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=======================================================================================================================** Find and return the calibration files which contain vendor supplied calibration information. This is achieved by searching the calibration directories and matching serial numbers to UIDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serial_nums = get_serial_nums(SPKIR, uids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serial_nums;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_dict = get_calibration_files(serial_nums, cal_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve and save the full directory path to the calibration files\n",
    "cal_paths = {}\n",
    "for uid in sorted(cal_dict.keys()):\n",
    "    paths = []\n",
    "    for file in cal_dict.get(uid):\n",
    "        path = generate_file_path(cal_directory, file, ext=['.zip','.cap', '.txt', '.log'])\n",
    "        paths.append(path)\n",
    "    cal_paths.update({uid: paths})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_paths;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=======================================================================================================================**\n",
    "# Parsing Calibration Coefficients\n",
    "Above, we have worked through identifying and mapping the calibration files and QCT check-ins to the individual instruments through their UIDs and serial numbers. The next step is to open the relevant files and parse out the calibration coefficients. This will require writing a parser for the SPKIR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPKIRCalibration():\n",
    "    # Class that stores calibration values for CTDs.\n",
    "\n",
    "    def __init__(self, uid):\n",
    "        self.serial = None\n",
    "        self.uid = uid\n",
    "        self.date = []\n",
    "        self.coefficients = {\n",
    "            'CC_immersion_factor': [],\n",
    "            'CC_offset': [],\n",
    "            'CC_scale': []\n",
    "        }\n",
    "        self.notes = {\n",
    "            'CC_immersion_factor': '',\n",
    "            'CC_offset': '',\n",
    "            'CC_scale': '',\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def uid(self):\n",
    "        return self._uid\n",
    "\n",
    "    @uid.setter\n",
    "    def uid(self, d):\n",
    "        r = re.compile('.{5}-.{6}-.{5}')\n",
    "        if r.match(d) is not None:\n",
    "            self._uid = d\n",
    "            self.serial = d.split('-')[-1].lstrip('0')\n",
    "        else:\n",
    "            raise Exception(f\"The instrument uid {d} is not a valid uid. Please check.\")\n",
    "            \n",
    "            \n",
    "    def load_cal(self, filepath):\n",
    "        \"\"\"\n",
    "        Wrapper function to load all of the calibration coefficients\n",
    "        \n",
    "        Args:\n",
    "            filepath - path to the directory with filename which has the\n",
    "                calibration coefficients to be parsed and loaded\n",
    "        Calls:\n",
    "            open_cal\n",
    "            parse_cal\n",
    "        \"\"\"\n",
    "        \n",
    "        data = self.open_cal(filepath)\n",
    "        \n",
    "        self.parse_cal(data)\n",
    "        \n",
    "        \n",
    "    def open_cal(self, filepath):\n",
    "        \"\"\"\n",
    "        Function that opens and reads in cal file\n",
    "        information for a SPKIR. Zipfiles are acceptable inputs.\n",
    "        \"\"\"\n",
    "        \n",
    "        if filepath.endswith('.zip'):\n",
    "            with ZipFile(filepath) as zfile:\n",
    "                # Check if OPTAA has the .dev file\n",
    "                filename = [name for name in zfile.namelist() if name.lower().endswith('.cal')]\n",
    "                \n",
    "                # Get and open the latest calibration file\n",
    "                if len(filename) == 1:\n",
    "                    data = zfile.read(filename[0]).decode('ascii')\n",
    "                    self.source_file(filepath, filename[0])\n",
    "                    \n",
    "                elif len(filename) > 1:\n",
    "                    raise FileExistsError(f\"Multiple .cal files found in {filepath}.\")\n",
    "\n",
    "                else:\n",
    "                    raise FileNotFoundError(f\"No .cal file found in {filepath}.\")\n",
    "                        \n",
    "        elif filepath.lower().endswith('.cal'):\n",
    "            with open(filepath) as file:\n",
    "                data = file.read()\n",
    "            self.source_file(filepath, file)\n",
    "          \n",
    "        else:\n",
    "            raise FileNotFoundError(f\"No .cal file found in {filepath}.\")\n",
    "        \n",
    "        return data\n",
    "        \n",
    "        \n",
    "    def parse_cal(self, data):\n",
    "        \n",
    "        flag = False\n",
    "        for line in data.splitlines():\n",
    "            if line.startswith('#'):\n",
    "                parts = line.split('|')\n",
    "                if len(parts) > 5 and 'Calibration' in parts[-1].strip():\n",
    "                    cal_date = parts[0].replace('#','').strip()\n",
    "                    self.date.append(pd.to_datetime(cal_date).strftime('%Y%m%d'))\n",
    "                    \n",
    "            elif line.startswith('SN'):\n",
    "                parts = line.split()\n",
    "                _, sn, *ignore = parts\n",
    "                sn = sn.lstrip('0')\n",
    "                if self.serial != sn:\n",
    "                    raise ValueError(f'Instrument serial number {sn} does not match UID {self.uid}')\n",
    "                    \n",
    "            elif line.startswith('ED'):\n",
    "                flag = True\n",
    "                \n",
    "            elif flag:\n",
    "                offset, scale, immersion_factor = line.split()\n",
    "                self.coefficients['CC_immersion_factor'].append(immersion_factor)\n",
    "                self.coefficients['CC_offset'].append(offset)\n",
    "                self.coefficients['CC_scale'].append(scale)\n",
    "                flag = False\n",
    "                \n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        \n",
    "    def source_file(self, filepath, filename):\n",
    "        \"\"\"\n",
    "        Routine which parses out the source file and filename\n",
    "        where the calibration coefficients are sourced from.\n",
    "        \"\"\"\n",
    "        \n",
    "        if filepath.lower().endswith('.cal'):\n",
    "            dcn = filepath.split('/')[-2]\n",
    "            filename = filepath.split('/')[-1]\n",
    "        else:\n",
    "            dcn = filepath.split('/')[-1]\n",
    "        \n",
    "        self.source = f'Source file: {dcn} > {filename}'\n",
    "        \n",
    "        \n",
    "    def write_csv(self, outpath):\n",
    "        \"\"\"\n",
    "        This function writes the correctly named csv file for the ctd to the\n",
    "        specified directory.\n",
    "\n",
    "        Args:\n",
    "            outpath - directory path of where to write the csv file\n",
    "        Raises:\n",
    "            ValueError - raised if the CTD object's coefficient dictionary\n",
    "                has not been populated\n",
    "        Returns:\n",
    "            self.to_csv - a csv of the calibration coefficients which is\n",
    "                written to the specified directory from the outpath.\n",
    "        \"\"\"\n",
    "\n",
    "        # Run a check that the coefficients have actually been loaded\n",
    "        if len(self.coefficients.values()) <= 2:\n",
    "            raise ValueError('No calibration coefficients have been loaded.')\n",
    "\n",
    "        # Create a dataframe to write to the csv\n",
    "        data = {\n",
    "            'serial': [self.serial]*len(self.coefficients),\n",
    "            'name': list(self.coefficients.keys()),\n",
    "            'value': list(self.coefficients.values())\n",
    "        }\n",
    "        df = pd.DataFrame().from_dict(data)\n",
    "      \n",
    "        # Now merge the coefficients dataframe with the notes\n",
    "        notes = pd.DataFrame().from_dict({\n",
    "            'name':list(self.notes.keys()),\n",
    "            'notes':list(self.notes.values())\n",
    "        })\n",
    "        df = df.merge(notes, how='outer', left_on='name', right_on='name')\n",
    "            \n",
    "        # Add in the source file\n",
    "        df['notes'].iloc[0] = df['notes'].iloc[0] + ' ' + self.source\n",
    "        \n",
    "        # Sort the data by the coefficient name\n",
    "        df = df.sort_values(by='name')\n",
    "\n",
    "        # Generate the csv names\n",
    "        csv_name = self.uid + '__' + max(self.date) + '.csv'\n",
    "        \n",
    "        # Write the dataframe to a csv file\n",
    "        check = input(f\"Write {csv_name} to {outpath}? [y/n]: \")\n",
    "        # check = 'y'\n",
    "        if check.lower().strip() == 'y':\n",
    "            df.to_csv(outpath+'/'+csv_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=======================================================================================================================**\n",
    "# Source Loading of Calibration Coefficients\n",
    "With a SPKIR Calibration object created, we can now begin parsing the different calibration sources for each SPKIR. We will then compare all of the calibration values from each of the sources, checking for any discrepancies between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, I plan on going through each of the SPKIR UIDs, and parse the data into csvs. For source files which may contain multiple calibrations or calibration sources, I plan on extracting each of the calibrations to a temporary folder using the following structure:\n",
    "\n",
    "    <local working directory>/<temp>/<source>/data/<calibration file>\n",
    "    \n",
    "The separate calibrations will be saved using the standard UFrame naming convention with the following directory structure:\n",
    "\n",
    "    <local working directory>/<temp>/<source>/<calibration csv>\n",
    "    \n",
    "The csvs themselves will also be copied to the temporary folder. This allows for the program to be looking into the same temp directory for every SPKIR check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid = uids[20]\n",
    "uid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the local temp directory. If it already exists; purge it and rewrite:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_directory = '/'.join((os.getcwd(),'temp'))\n",
    "if os.path.exists(temp_directory):\n",
    "    shutil.rmtree(temp_directory)\n",
    "    ensure_dir(temp_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the existing csvs from asset management to the temp directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in csv_paths[uid]:\n",
    "    savedir = '/'.join((temp_directory,'csv'))\n",
    "    ensure_dir(savedir)\n",
    "    savepath = '/'.join((savedir, path.split('/')[-1]))\n",
    "    shutil.copyfile(path, savepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=======================================================================================================================**\n",
    "Load the calibration coefficients from the vendor calibration source files. Start by extracting or copying them to the source data folder in the temporary directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the calibration zip files to the local temp directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in cal_paths[uid]:\n",
    "    with ZipFile(path) as zfile:\n",
    "        files = [name for name in zfile.namelist() if name.lower().endswith('.cal')]\n",
    "        for file in files:\n",
    "            exdir = path.split('/')[-1].strip('.zip')\n",
    "            expath = '/'.join((temp_directory,'cal','data',exdir))\n",
    "            ensure_dir(expath)\n",
    "            zfile.extract(file,path=expath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the vendor calibration files to csvs following the UFrame convention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savedir = '/'.join((temp_directory,'cal'))\n",
    "ensure_dir(savedir)\n",
    "# Now parse the calibration coefficients\n",
    "for dirpath, dirnames, filenames in os.walk('/'.join((temp_directory,'cal','data'))):\n",
    "    for file in filenames:\n",
    "        filepath = os.path.join(dirpath, file)\n",
    "        # With the filepath for the given calibration retrived, I can now start an instance of the NUTNR Calibration\n",
    "        # object and begin parsing the coefficients\n",
    "        spkir = SPKIRCalibration(uid)\n",
    "        spkir.load_cal(filepath)\n",
    "        spkir.write_csv(savedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=======================================================================================================================**\n",
    "# Calibration Coefficient Comparison\n",
    "We have now successfully parsed the calibration files from all the possible sources: the vendor calibration files, the pre-deployments files, and the post-deployment files. Furthermore, we have saved csvs in the UFrame format for all of these calibrations. Now, we want to load those csvs into pandas dataframes, which allow for easy element-by-element comparison of calibration coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_date(x):\n",
    "    x = str(x)\n",
    "    ind1 = x.index('__')\n",
    "    ind2 = x.index('.')\n",
    "    return x[ind1+2:ind2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to compare dataframe\n",
    "csv_files = [file for file in sorted(os.listdir('temp/csv')) if 'data' not in file]\n",
    "csv_files = pd.DataFrame(csv_files, columns=['csv'])\n",
    "csv_files['cal date'] = csv_files['csv'].apply(lambda x: get_file_date(x))\n",
    "csv_files.set_index('cal date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to compare dataframe\n",
    "cal_files = [file for file in sorted(os.listdir('temp/cal')) if 'data' not in file]\n",
    "cal_files = pd.DataFrame(cal_files, columns=['cal'])\n",
    "cal_files['cal date'] = cal_files['cal'].apply(lambda x: get_file_date(x))\n",
    "cal_files.set_index('cal date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_files = csv_files.join(cal_files,how='outer').fillna(value='-999')\n",
    "df_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename above CSV file names\n",
    "sn = '00302'\n",
    "d1 = '20170913'\n",
    "d2 = '20170911'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = 'temp/csv/' + f'CGINS-SPKIRB-{sn}__{d1}.csv'\n",
    "dst = 'temp/csv/' + f'CGINS-SPKIRB-{sn}__{d2}.csv'\n",
    "shutil.move(src, dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload the csv files in order to perform the comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV files\n",
    "csv_files = [file for file in sorted(os.listdir('temp/csv')) if 'data' not in file]\n",
    "csv_files = pd.DataFrame(csv_files, columns=['csv'])\n",
    "csv_files['cal date'] = csv_files['csv'].apply(lambda x: get_file_date(x))\n",
    "csv_files.set_index('cal date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration source files\n",
    "cal_files = [file for file in sorted(os.listdir('temp/cal')) if 'data' not in file]\n",
    "cal_files = pd.DataFrame(cal_files, columns=['cal'])\n",
    "cal_files['cal date'] = cal_files['cal'].apply(lambda x: get_file_date(x))\n",
    "cal_files.set_index('cal date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_files = csv_files.join(cal_files,how='outer').fillna(value='-999')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=======================================================================================================================**\n",
    "Now, with the csv files renamed to match their associated calibration dates following the OOI UFrame format, we can load the info into pandas dataframe which will allow for the direct comparison of calibration coefficients using built in array comparison tools from numpy. \n",
    "\n",
    "A complication is that, when loading a csv using pandas, it reads the csv as strings. This includes characters such as **[]**. Consequently, we need to reformat the arrays in the dataframe and convert to 64-bit floating point numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_arrays(array):\n",
    "    # First, need to strip extraneous characters from the array\n",
    "    array = array.replace(\"'\",\"\").replace('[','').replace(']','')\n",
    "    # Next, split the array into a list\n",
    "    array = array.split(',')\n",
    "    # Now, need to eliminate any white space surrounding the individual coeffs\n",
    "    array = [num.strip() for num in array]\n",
    "    # Next, float the nums\n",
    "    try:\n",
    "        array = [float(num) for num in array]\n",
    "        # Check if the array is len == 1; if so, can just return the number\n",
    "        if len(array) == 1:\n",
    "            array = array[0]\n",
    "    except:\n",
    "        pass\n",
    "    # Now we are done\n",
    "    return array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the calibration coefficients into pandas dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use fstring literals to allow on the fly file-renaming\n",
    "dt = '20170911'\n",
    "fname = f'CGINS-SPKIRB-{sn}__{dt}.csv'2717"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV = pd.read_csv('temp/csv/'+fname)\n",
    "CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAL = pd.read_csv('temp/cal/'+fname)\n",
    "CAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat the arrays\n",
    "CSV['value'] = CSV['value'].apply(lambda x: reformat_arrays(x))\n",
    "CAL['value'] = CAL['value'].apply(lambda x: reformat_arrays(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the calibration coefficients agree\n",
    "np.equal(CSV,CAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the source file for the calibration coefficients\n",
    "CAL['notes'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
