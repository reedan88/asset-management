{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Analysis for Metadata Review in OOI Asset Management System\n",
    "\n",
    "### Motivation:\n",
    "The Asset Management system for OOI is primarly housed on GitHub in a variety of csv files. Until now, the calibration coefficients stored in the csv files have been manually entered. While we have utilized a \"human-in-the-loop\" review approach to catch errors, some errors have slipped through (e.g. truncation of significant figures).\n",
    "\n",
    "### Approach:\n",
    "My goal is to develop an automated approach to catch possible errors which already exist within the asset management system. To accomplish this, I will compare the csv files loaded into the GitHub asset management system with the original vendor files as well as the QCT (quality control testing) documents which capture the coefficients loaded onto the instrument at the time of reception at WHOI from the vendor.\n",
    "\n",
    "### Data Sources:\n",
    "* **GitHub**: CSV files containing the calibration coefficients. Directory organization by sensor+class. The files are named as \"(CGINS)-(sensor+class)-(serial number)-(YYYYMMDD)\" where YYYYMMDD is the calibration date.\n",
    "* **Vault**: Version-controlled storage location of the vendor calibrations, in the Records/Instrument Records/Instrument directories. Within the relevant directory, calibration files are stored as either .cal, .xmlcon, .pdf, or within zipped directories.\n",
    "* **Alfresco**: Version-controlled web-accessed. The calibrations loaded onto the instrument during the initial checkin-in upon receipt (the QCT process) are stored here as either .cap or .txt files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import likely important packages, etc.\n",
    "import sys, os, csv, re\n",
    "from wcmatch import fnmatch\n",
    "import datetime\n",
    "import time\n",
    "import xml.etree.ElementTree as et\n",
    "from zipfile import ZipFile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/'.join((os.getcwd(),'Calibration','Parsers')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import self-written functions from utils package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from CTDCalibration import CTDCalibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WHOI Asset Tracking Spreadsheet\n",
    "First, I want to load and examine exactly what type of data is stored in the WHOI Asset Tracking Spreadsheet and what information it has that may be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#excel_spreadsheet = 'C:/Users/areed/Documents/Project_Files/Documentation/System/System Notebook/WHOI_Asset_Tracking.xlsx'\n",
    "excel_spreadsheet = '/media/andrew/OS/Users/areed/Documents/Project_Files/Documentation/System/System Notebook/WHOI_Asset_Tracking.xlsx'\n",
    "sheet_name = 'Sensors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are all the different series of CTDs?\n",
    "ADCPA = whoi_asset_tracking(excel_spreadsheet,sheet_name,instrument_class='ADCPA',whoi=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADCPA.dropna(subset=['UID'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Instrument\n",
       "Class</th>\n",
       "      <th>Series</th>\n",
       "      <th>Supplier\n",
       "Serial Number</th>\n",
       "      <th>WHOI #</th>\n",
       "      <th>OOI #</th>\n",
       "      <th>UID</th>\n",
       "      <th>Model</th>\n",
       "      <th>CGSN PN</th>\n",
       "      <th>Firmware Version</th>\n",
       "      <th>Supplier</th>\n",
       "      <th>...</th>\n",
       "      <th>QCT Testing</th>\n",
       "      <th>PreDeployment</th>\n",
       "      <th>Post Deployment</th>\n",
       "      <th>Refurbishment/ Repair</th>\n",
       "      <th>DO Number</th>\n",
       "      <th>Date Received</th>\n",
       "      <th>Deployment History</th>\n",
       "      <th>Current Deployment</th>\n",
       "      <th>Instrument Location on Current Deployment</th>\n",
       "      <th>Notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Instrument\n",
       "Class, Series, Supplier\n",
       "Serial Number, WHOI #, OOI #, UID, Model, CGSN PN, Firmware Version, Supplier, Current\n",
       "Owner, Current\n",
       "Location, Status, Latest Calibration Date, Incoming Inspection, QCT Testing, PreDeployment, Post Deployment, Refurbishment/ Repair, DO Number, Date Received, Deployment History, Current Deployment, Instrument Location on Current Deployment, Notes]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 25 columns]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ADCPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'F', 'G'}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(ADCPT['Series'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(ADCPT['UID']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "qct_directory = '/media/andrew/OS/Users/areed/Documents/Project_Files/Records/Instrument_Records/ADCPT/ADCPT_Results'\n",
    "cal_directory = '/media/andrew/OS/Users/areed/Documents/Project_Files/Records/Instrument_Records/ADCPT/ADCPT_Cal'\n",
    "asset_management_directory = '/home/andrew/Documents/OOI-CGSN/ooi-integration/asset-management/calibration/ADCPTG/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CGINS-ADCPTG-18596': ['CGINS-ADCPTG-18596__20161002.csv',\n",
       "  'CGINS-ADCPTG-18596__20140416.csv'],\n",
       " 'CGINS-ADCPTG-18660': ['CGINS-ADCPTG-18660__20141009.csv',\n",
       "  'CGINS-ADCPTG-18660__20160520.csv',\n",
       "  'CGINS-ADCPTG-18660__20170112.csv',\n",
       "  'CGINS-ADCPTG-18660__20150501.csv',\n",
       "  'CGINS-ADCPTG-18660__20180307.csv'],\n",
       " 'CGINS-ADCPTG-19151': ['CGINS-ADCPTG-19151__20140413.csv',\n",
       "  'CGINS-ADCPTG-19151__20160701.csv',\n",
       "  'CGINS-ADCPTG-19151__20170214.csv',\n",
       "  'CGINS-ADCPTG-19151__20160521.csv',\n",
       "  'CGINS-ADCPTG-19151__20150430.csv'],\n",
       " 'CGINS-ADCPTG-19336': ['CGINS-ADCPTG-19336__20131123.csv',\n",
       "  'CGINS-ADCPTG-19336__20141007.csv',\n",
       "  'CGINS-ADCPTG-19336__20151017.csv'],\n",
       " 'CGINS-ADCPTG-20495': ['CGINS-ADCPTG-20495__20151015.csv',\n",
       "  'CGINS-ADCPTG-20495__20161003.csv',\n",
       "  'CGINS-ADCPTG-20495__20141011.csv'],\n",
       " 'CGINS-ADCPTG-20496': ['CGINS-ADCPTG-20496__20160521.csv',\n",
       "  'CGINS-ADCPTG-20496__20150503.csv',\n",
       "  'CGINS-ADCPTG-20496__20170214.csv'],\n",
       " 'CGINS-ADCPTG-20499': ['CGINS-ADCPTG-20499__20161004.csv',\n",
       "  'CGINS-ADCPTG-20499__20151016.csv',\n",
       "  'CGINS-ADCPTG-20499__20140412.csv'],\n",
       " 'CGINS-ADCPTG-24542': ['CGINS-ADCPTG-24542__20170202.csv'],\n",
       " 'CGINS-ADCPTG-24543': ['CGINS-ADCPTG-24543__20170202.csv'],\n",
       " 'CGINS-ADCPTG-24544': ['CGINS-ADCPTG-24544__20170202.csv']}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_dict = load_asset_management(ADCPT, asset_management_directory)\n",
    "csv_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(csv_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n"
     ]
    }
   ],
   "source": [
    "#count = 0\n",
    "for uid in csv_dict.keys():\n",
    "    count = count + len(csv_dict[uid])\n",
    "print(count)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load an example of the csv file to determine the number of calibration coefficients per instrument that need to be checked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = generate_file_path(asset_management_directory, csv_dict['CGINS-SPKIRB-00288'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/andrew/Documents/OOI-CGSN/ooi-integration/asset-management/calibration/SPKIRB/CGINS-SPKIRB-00288__20160921.csv'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>serial</th>\n",
       "      <th>name</th>\n",
       "      <th>value</th>\n",
       "      <th>notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>288</td>\n",
       "      <td>CC_immersion_factor</td>\n",
       "      <td>[1.368, 1.41, 1.365, 1.354, 1.372, 1.322, 1.347]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>288</td>\n",
       "      <td>CC_offset</td>\n",
       "      <td>[2148860972.1, 2147208222.4, 2147372698.2, 214...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>288</td>\n",
       "      <td>CC_scale</td>\n",
       "      <td>[2.05971500566e-07, 2.02581099515e-07, 2.03984...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   serial                 name  \\\n",
       "0     288  CC_immersion_factor   \n",
       "1     288            CC_offset   \n",
       "2     288             CC_scale   \n",
       "\n",
       "                                               value  notes  \n",
       "0   [1.368, 1.41, 1.365, 1.354, 1.372, 1.322, 1.347]    NaN  \n",
       "1  [2148860972.1, 2147208222.4, 2147372698.2, 214...    NaN  \n",
       "2  [2.05971500566e-07, 2.02581099515e-07, 2.03984...    NaN  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(fpath)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the unique identifiers (UID) of the instruments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uids = list(set(DOSTA['UID']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(uids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qct_dict = {}\n",
    "for uid in uids:\n",
    "    # Get the QCT Document numbers from the asset tracking sheet\n",
    "    CTDBP['UID_match'] = CTDBP['UID'].apply(lambda x: True if uid in x else False)\n",
    "    qct_series = CTDBP[CTDBP['UID_match'] == True]['QCT Testing']\n",
    "    qct_series = list(qct_series.iloc[0].split('\\n'))\n",
    "    qct_dict.update({uid:qct_series})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serial_nums = get_serial_nums(CTDBP, uids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the calibration files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_files = {}\n",
    "for uid in serial_nums.keys():\n",
    "    sn = serial_nums.get(uid)\n",
    "    sn = str(sn[:])\n",
    "    files = []\n",
    "    for file in os.listdir(cal_directory):\n",
    "        if 'Calibration_File' in file:\n",
    "            if sn in file:\n",
    "                files.append(file)\n",
    "    calibration_files.update({uid:files})\n",
    "cal_dict = calibration_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purge the temp directory\n",
    "shutil.rmtree('/'.join((os.getcwd(),'temp')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the csv files into a similar temp directory for local working\n",
    "for file in csv_dict[uid]:\n",
    "    csv_savepath = '/'.join((os.getcwd(),'temp','csv'))\n",
    "    ensure_dir('/'.join((os.getcwd(),'temp','csv')))\n",
    "    # Now save the csv into the temp directory\n",
    "    shutil.copy('/'.join((asset_management_directory,file)), csv_savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_file_path(dirpath, filename, ext=['.cap', '.txt', '.log'], exclude=['_V', '_Data_Workshop']):\n",
    "    \"\"\"\n",
    "    Function which searches for the location of the given file and returns\n",
    "    the full path to the file.\n",
    "\n",
    "    Args:\n",
    "        dirpath - parent directory path under which to search\n",
    "        filename - the name of the file to search for\n",
    "        ext - file endings to search for\n",
    "        exclude - optional list which allows for excluding certain\n",
    "            directories from the search\n",
    "    Returns:\n",
    "        fpath - the file path to the filename from the current\n",
    "            working directory.\n",
    "    \"\"\"\n",
    "    # Check if the input file name has an extension already\n",
    "    # If it does, parse it for input into the search algo\n",
    "    if '.' in filename:\n",
    "        check = filename.split('.')\n",
    "        filename = check[0]\n",
    "        ext = ['.'+check[1]]\n",
    "\n",
    "    for root, dirs, files in os.walk(dirpath):\n",
    "        dirs[:] = [d for d in dirs if d not in exclude]\n",
    "        for fname in files:\n",
    "            if fnmatch.fnmatch(fname, [filename+'*'+x for x in ext]):\n",
    "                fpath = os.path.join(root, fname)\n",
    "                return fpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in qct_dict[uid]:\n",
    "    # Generate the full file path\n",
    "    qct_path = generate_file_path(qct_directory, file)\n",
    "    # Initialize a CTD object\n",
    "    CTD = CTDCalibration(uid=uid)\n",
    "    # Load the QCT information\n",
    "    try:\n",
    "        CTD.load_qct(qct_path)\n",
    "        # Generate the save file path\n",
    "        qct_savepath = '/'.join((os.getcwd(),'temp','qct'))\n",
    "        ensure_dir('/'.join((os.getcwd(),'temp','qct')))\n",
    "    except ValueError:\n",
    "        print(f\"QCT UID does not match serial in QCT\")\n",
    "    except:\n",
    "        print(f'No QCT file found for: {file}')\n",
    "    # Now save the qct info to a csv\n",
    "    try:\n",
    "        CTD.write_csv(qct_savepath)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in cal_dict[uid]:\n",
    "    # Generate the full file path\n",
    "    cal_path = generate_file_path(cal_directory, file, ext=[''])\n",
    "    # Initialize a CTD object\n",
    "    CTD = CTDCalibration(uid=uid)\n",
    "    # Load the QCT information\n",
    "    xml_savepath = '/'.join((os.getcwd(),'temp','xml'))\n",
    "    ensure_dir(xml_savepath)\n",
    "    try:\n",
    "        CTD.load_xml(cal_path)\n",
    "        # Generate the save file path\n",
    "        xml_savepath = '/'.join((os.getcwd(),'temp','xml'))\n",
    "        ensure_dir(xml_savepath)\n",
    "        # Now save the qct info to a csv\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        CTD.write_csv(xml_savepath)\n",
    "    except ValueError:\n",
    "        print(f'No xml file found for {file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in cal_dict[uid]:\n",
    "    # Generate the full file path\n",
    "    cal_path = generate_file_path(cal_directory, file, ext=[''])\n",
    "    # Initialize a CTD object\n",
    "    CTD = CTDCalibration(uid=uid)\n",
    "    # Load the QCT information\n",
    "    try:\n",
    "        CTD.load_cal(cal_path)\n",
    "        # Generate the save file path\n",
    "        cal_savepath = '/'.join((os.getcwd(),'temp','cal'))\n",
    "        ensure_dir(cal_savepath)\n",
    "        # Now save the qct info to a csv\n",
    "    except ValueError:\n",
    "        raise ValueError\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        CTD.write_csv(cal_savepath)\n",
    "    except ValueError:\n",
    "        print(f'No cal file found for {file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CTD.serial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking instrument calibration values\n",
    "After loading the **WHOI Asset Tracking Sheet**, we now have the following critical data for checking calibration information:\n",
    "* Supplier Serial Number - this links back to the original **.cal**, **.xmlcon**, and vendor docs\n",
    "* OOI UID - this is the link between the instrument and the OOINet\n",
    "* QCT Document Number - this number links the instrument to the QCT screen capture of the calibration values loaded onto the instruments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process to load the **CSV** calibration file\n",
    "In order to check that the calibrations in asset management, I have to be able to load the asset management calibration csv files into a dataframe. \n",
    "* First, get all the unique CTDBPCs in Asset Management\n",
    "* Next, parse the csv files in asset management to get the unique instrument serial numbers\n",
    "* With the serial numbers, find the associated instrument calibration csvs\n",
    "* For each calibration csv, load the data into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_date(x):\n",
    "    x = str(x)\n",
    "    ind1 = x.index('__')\n",
    "    ind2 = x.index('.')\n",
    "    return x[ind1+2:ind2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to compare dataframe\n",
    "csv_files = pd.DataFrame(sorted(csv_dict[uid]),columns=['csv'])\n",
    "csv_files['cal date'] = csv_files['csv'].apply(lambda x: get_file_date(x))\n",
    "csv_files.set_index('cal date',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to compare dataframe\n",
    "cal_files = pd.DataFrame(sorted(os.listdir('temp/cal')),columns=['cal'])\n",
    "cal_files['cal date'] = cal_files['cal'].apply(lambda x: get_file_date(x))\n",
    "cal_files.set_index('cal date',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to compare dataframe\n",
    "xml_files = pd.DataFrame(sorted(os.listdir('temp/xml')),columns=['xml'])\n",
    "xml_files['cal date'] = xml_files['xml'].apply(lambda x: get_file_date(x))\n",
    "xml_files.set_index('cal date',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to compare dataframe\n",
    "qct_files = pd.DataFrame(sorted(os.listdir('temp/qct')),columns=['qct'])\n",
    "qct_files['cal date'] = qct_files['qct'].apply(lambda x: get_file_date(x))\n",
    "qct_files.set_index('cal date',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_files = csv_files.join(cal_files,how='outer').join(xml_files,how='outer').join(qct_files,how='outer').fillna(value='-999')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CTD.ctd_type + '-' + CTD.serial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_files['csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_cal_coeffs(coeffs_dict):\n",
    "    \n",
    "    # Part 1: coeff by coeff comparison between each source of coefficients\n",
    "    keys = list(coeffs_dict.keys())\n",
    "    comparison = {}\n",
    "    for i in range(len(keys)):\n",
    "        names = (keys[i], keys[i - (len(keys)-1)])\n",
    "        check = len(coeffs_dict.get(keys[i])['value']) == len(coeffs_dict.get(keys[i - (len(keys)-1)])['value'])\n",
    "        if check:\n",
    "            compare = np.isclose(coeffs_dict.get(keys[i])['value'], coeffs_dict.get(keys[i - (len(keys)-1)])['value'])\n",
    "            comparison.update({names:compare})\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    # Part 2: now do a logical_and comparison between the results from part 1\n",
    "    keys = list(comparison.keys())\n",
    "    i = 0\n",
    "    mask = comparison.get(keys[i])\n",
    "    while i < len(keys)-1:\n",
    "        i = i + 1\n",
    "        mask = np.logical_and(mask, comparison.get(keys[i]))\n",
    "        print(i)\n",
    "       \n",
    "    return mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {}\n",
    "for cal_date in df_files.index:\n",
    "    # Part 1, load all of the csv files\n",
    "    coeffs_dict = {}\n",
    "    for source,fname in df_files.loc[cal_date].items():\n",
    "        if fname != '-999':\n",
    "            load_directory = '/'.join((os.getcwd(),'temp',source,fname))\n",
    "            df_coeffs = pd.read_csv(load_directory)\n",
    "            df_coeffs.set_index(keys='name',inplace=True)\n",
    "            df_coeffs.sort_index(inplace=True)\n",
    "            coeffs_dict.update({source:df_coeffs})\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    # Part 2, now check the calibration coefficients\n",
    "    mask = check_cal_coeffs(coeffs_dict)\n",
    "    \n",
    "    # Part 3: get the calibration coefficients are wrong\n",
    "    # and show them\n",
    "    fname = df_files.loc[cal_date]['csv']\n",
    "    if fname == '-999':\n",
    "        incorrect = 'No csv file.'\n",
    "    else:\n",
    "        incorrect = coeffs_dict['csv'][mask == False]\n",
    "    result.update({fname:incorrect})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV = pd.read_csv('/'.join((asset_management_directory,df_files.loc[indices[0]].loc['csv'])))\n",
    "CSV.set_index(keys='name',inplace=True)\n",
    "CSV.sort_index(inplace=True)\n",
    "CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QCT = pd.read_csv('/'.join((os.getcwd(),'temp','qct',df_files.loc[indices[0]].loc['qct'])))\n",
    "QCT.set_index(keys='name',inplace=True)\n",
    "QCT.sort_index(inplace=True)\n",
    "QCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XML = pd.read_csv('/'.join((os.getcwd(),'temp','xml',df_files.loc[indices[0]].loc['xml'])))\n",
    "XML.set_index(keys='name',inplace=True)\n",
    "XML.sort_index(inplace=True)\n",
    "XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_xml = np.isclose(CSV['value'],XML['value'])\n",
    "csv_qct = np.isclose(CSV['value'],QCT['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (csv_xml | csv_qct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV[mask == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have successfully loaded the csv calibrations into a pandas dataframe that allows for easy comparison between calibrations based on the calibration date for each calibration coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the QCT values\n",
    "The next step is to take the capture files from the QCT and load them into a comparable pandas dataframe. This involves several steps:\n",
    "* Get the QCT document numbers from the WHOI Asset Tracking Sheet for each individual instrument\n",
    "* Find where the QCT documents are stored\n",
    "* Load the QCT documents\n",
    "* Parse the QCT documents\n",
    "* Translate the parsed QCT values into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uids = sorted( list( set( CTDBPP['UID'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I need to load the all of the csv files based on their UID\n",
    "def load_csv_info(csv_dict,filepath):\n",
    "    \"\"\"\n",
    "    Loads the calibration coefficient information contained in asset management\n",
    "    \n",
    "    Args:\n",
    "        csv_dict - a dictionary which associates an instrument UID to the\n",
    "            calibration csv files in asset management\n",
    "        filepath - the path to the directory containing the calibration csv files\n",
    "    Returns:\n",
    "        csv_cals - a dictionary which associates an instrument UID to a pandas\n",
    "            dataframe which contains the calibration coefficients. The dataframes\n",
    "            are indexed by the date of calibration\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the calibration data into pandas dataframes, which are then placed into\n",
    "    # a dictionary by the UID\n",
    "    csv_cals = {}\n",
    "    for uid in csv_dict:\n",
    "        cals = pd.DataFrame()\n",
    "        for file in csv_dict[uid]:\n",
    "            data = pd.read_csv(filepath+file)\n",
    "            date = file.split('__')[1].split('.')[0]\n",
    "            data['CAL DATE'] = pd.to_datetime(date)\n",
    "            cals = cals.append(data)\n",
    "        csv_cals.update({uid:cals})\n",
    "        \n",
    "    # Pivot the dataframe to be sorted based on calibration date\n",
    "    for uid in csv_cals:\n",
    "        csv_cals[uid] = csv_cals[uid].pivot(index=csv_cals[uid]['CAL DATE'], columns='name')['value']\n",
    "        \n",
    "    return csv_cals\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qct_dict = {}\n",
    "for uid in uids:\n",
    "    # Get the QCT Document numbers from the asset tracking sheet\n",
    "    CTDBPP['UID_match'] = CTDBPP['UID'].apply(lambda x: True if uid in x else False)\n",
    "    qct_series = CTDBPP[CTDBPP['UID_match'] == True]['QCT Testing']\n",
    "    qct_series = list(qct_series.iloc[0].split('\\n'))\n",
    "    qct_dict.update({uid:qct_series})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qct_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try building a function to do the file path generator\n",
    "def generate_file_path(dirpath,filename,ext=['.cap','.txt','.log'],exclude=['_V','_Data_Workshop']):\n",
    "    \"\"\"\n",
    "    Function which searches for the location of the given file and returns\n",
    "    the full path to the file.\n",
    "    \n",
    "    Args:\n",
    "        dirpath - parent directory path under which to search\n",
    "        filename - the name of the file to search for\n",
    "        ext - \n",
    "        exclude - optional list which allows for excluding certain\n",
    "            directories from the search\n",
    "    Returns:\n",
    "        fpath - the file path to the filename from the current\n",
    "            working directory.\n",
    "    \"\"\"\n",
    "    for root, dirs, files in os.walk(dirpath):\n",
    "        dirs[:] = [d for d in dirs if d not in exclude]\n",
    "        for fname in files:\n",
    "            if fnmatch.fnmatch(fname, [filename+'*'+x for x in ext]):\n",
    "                fpath = os.path.join(root, fname)\n",
    "                return fpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qct_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CTD = CTDCalibration(uid=uids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CTD.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CTD.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CTD.load_qct(qct_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CTD.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qct_filepath = generate_file_path(dirpath,qcts[0])\n",
    "qct_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CTD = CTDCalibration(uid=uids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CTD.load_qct('/media/andrew/OS/Users/areed/Documents/Project_Files/Records/Instrument_Records/3305-00102-00019-A.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CTD.serial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, dirs, files in os.walk(dirpath):\n",
    "    dirs[:] = [d for d in dirs if d not in exclude]\n",
    "    for fname in files:\n",
    "        if fnmatch.fnmatch(fname, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to develop an automated approach to load all the QCT documents, parse them\n",
    "# into a dictionary, and convert the dictionary into a pandas dataframe\n",
    "def load_qct_data(qct_dict,coefficient_name_map,dirpath='../../../Documents/Project_Files/'):\n",
    "    qct = {}\n",
    "    qct_missing = {}\n",
    "    for uid in qct_dict:\n",
    "        print(uid)\n",
    "        capture_data = {}\n",
    "        missing = []\n",
    "        for capfile in qct_dict[uid]:\n",
    "            # First, find and return the path to the capture file which\n",
    "            # matches the capture file indentifier\n",
    "            cappath = generate_file_path(dirpath, capfile)\n",
    "            \n",
    "            # Function to pull out the coefficients from the capture files. This is a naive implementation\n",
    "            # and splits only on either a \":\" or \"=\", it doesn't do any comprehension of the file\n",
    "            if cappath is None:\n",
    "                missing.append(capfile)\n",
    "            else:\n",
    "                coeffs = {}\n",
    "                with open(cappath) as filename:\n",
    "                    data = filename.read()\n",
    "                    for line in data.splitlines():\n",
    "                        items = re.split(': | =',line)\n",
    "                        key = items[0].strip()\n",
    "                        value = items[-1].strip()\n",
    "                        coeffs.update({key:value})\n",
    "                    \n",
    "                # The best way to do this is to use the CTD name mapping to only get the important values\n",
    "                capture = {}\n",
    "                # With the capture coefficients, now map it to the CTD coefficients\n",
    "                for key in coeffs.keys():\n",
    "                    if key in coefficient_name_map.keys():\n",
    "                        capture[coefficient_name_map[key]] = coeffs[key]\n",
    "            \n",
    "                # Get the calibration date\n",
    "                caldate = coeffs['conductivity']\n",
    "            \n",
    "                # Update the capture file to include the calibration date\n",
    "                capture['CAL DATE'] = pd.to_datetime(caldate)\n",
    "            \n",
    "                # Now, update the parent dictionary\n",
    "                capture_data.update({capfile:capture})\n",
    "            \n",
    "        df = pd.DataFrame.from_dict({i: capture_data[i] for i in capture_data.keys()}, orient='index')\n",
    "        qct.update({uid:df})\n",
    "        qct_missing.update({uid:missing})\n",
    "        \n",
    "    return qct, qct_missing   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qct, qct_missing = load_qct_data(qct_dict,coefficient_name_map,dirpath='../../../../Documents/Project_Files/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qct_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index to the calibration date\n",
    "for uid in qct:\n",
    "    qct[uid].set_index('CAL DATE', drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vendor Calibration values: .cal and .xmlcon\n",
    "This next step is to load the CTD .cal and .xmlcon files in order to compare the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serial_nums = get_serial_nums(CTDBPC, uids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serial_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vendor_files = {}\n",
    "for uid,sn in serial_nums.items():\n",
    "    files = []\n",
    "    for file in os.listdir('../../../../Documents/Project_Files/Records/Instrument_Records/CTDBP/'):\n",
    "        if sn in file:\n",
    "            if 'Calibration_File' in file:\n",
    "                files.append(file)\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            pass\n",
    "    vendor_files.update({uid:files})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_dict = get_calibration_files(serial_nums,'/media/andrew/OS/Users/areed/Documents/Project_Files/Records/Instrument_Records/CTDBP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal = {}\n",
    "cal_missing = {}\n",
    "filepath = '../../../../Documents/Project_Files/Records/Instrument_Records/CTDBP/'\n",
    "for uid,files in vendor_files.items():\n",
    "    cal_coeffs, missing = load_cal_coeffs(files,filepath,coefficient_name_map,o2_coefficients_map)\n",
    "    cal_df = pd.DataFrame.from_dict({i: cal_coeffs[i] for i in cal_coeffs.keys()}, orient='index')\n",
    "    cal_df.index = pd.to_datetime(cal_df.index)\n",
    "    cal.update({uid:cal_df})\n",
    "    cal_missing.update({uid:missing})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repeat the above process with the .xmlcon file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml = {}\n",
    "xml_missing = {}\n",
    "filepath = '../../../../Documents/Project_Files/Records/Instrument_Records/CTDBP/'\n",
    "for uid,files in vendor_files.items():\n",
    "    xml_coeffs, missing = load_xml_coeffs(files,filepath,coefficient_name_map,o2_coefficients_map)\n",
    "    xml_df = pd.DataFrame.from_dict({i: xml_coeffs[i] for i in xml_coeffs.keys()}, orient='index')\n",
    "    xml_df.drop(columns=[None],axis=1,inplace=True)\n",
    "    xml_df.index = pd.to_datetime(xml_df.index)\n",
    "    xml.update({uid:xml_df})\n",
    "    xml_missing.update({uid:missing})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparisons\n",
    "Now that I have .cal, .xmlcon, the qct capture files, and the csv files from asset management, I can begin comparison of the calibration coefficients between the different files. The goal is that the dates, values, and coefficients all match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, I need to reindex all of the different dataframes such that they all have two indices:\n",
    "# A dataset index and a datetime index, and set them to uniform name (for concatenation)\n",
    "for uid in uids:\n",
    "    try:\n",
    "        CSV[uid]['Dataset'] = 'CSV'\n",
    "        CSV[uid].set_index(['Dataset',CSV[uid].index],inplace=True)\n",
    "        CSV[uid].index.set_names(['Dataset','Cal Date'],inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for uid in uids:\n",
    "    qct[uid]['Dataset'] = 'QCT'\n",
    "    qct[uid].set_index(['Dataset',qct[uid].index],inplace=True)\n",
    "    qct[uid].index.set_names(['Dataset','Cal Date'],inplace=True)\n",
    "qct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for uid in uids:\n",
    "    cal[uid]['Dataset'] = 'CAL'\n",
    "    cal[uid].set_index(['Dataset',cal[uid].index],inplace=True)\n",
    "    cal[uid].index.set_names(['Dataset','Cal Date'],inplace=True)\n",
    "cal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for uid in uids:\n",
    "    xml[uid]['Dataset'] = 'XML'\n",
    "    xml[uid].set_index(['Dataset',xml[uid].index],inplace=True)\n",
    "    xml[uid].index.set_names(['Dataset','Cal Date'],inplace=True)\n",
    "xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All four possible sources of calibration coefficients available for an instrument - the calibration **CSV** loaded into asset management, the calibration coefficients loaded onto the instrument during check-in (**QCT**), the **.cal** file provided by the vendor, and the **XML** file provided by the vendor. \n",
    "\n",
    "The next step is to concatenate the different instruments into a single dataframe and to sort by calibration date. This will allow for comparison based on the date of the calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = {}\n",
    "for uid in uids:\n",
    "    comparison.update({uid:pd.concat([CSV.get(uid), cal.get(uid), xml.get(uid), qct.get(uid)])})\n",
    "    comparison[uid].reset_index(level='Cal Date',inplace=True)\n",
    "    comparison[uid].sort_values(by='Cal Date',inplace=True)\n",
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_type(x):\n",
    "    if type(x) is str:\n",
    "        return float(x)\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for uid in uids:\n",
    "    comparison[uid] = comparison[uid].applymap(convert_type)\n",
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_the_same(elements):\n",
    "    \"\"\"\n",
    "    This function checks which values in an array are all the same.\n",
    "    \n",
    "    Args:\n",
    "        elements - an array of values\n",
    "    Returns:\n",
    "        error - an array of length (m-1) which checks if\n",
    "    \n",
    "    \"\"\"\n",
    "    if len(elements) < 1:\n",
    "        return True\n",
    "    el = iter(elements)\n",
    "    first = next(el, None)\n",
    "    #check = [element == first for element in el]\n",
    "    error = [np.isclose(element,first) for element in el]\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locate_cal_error(array):\n",
    "    \"\"\"\n",
    "    This function locates which source file (e.g. xmlcon vs csv vs cal)\n",
    "    have calibration values that are different from the others. It does\n",
    "    NOT identify which is correct, only which is different.\n",
    "    \n",
    "    Args:\n",
    "        array - A numpy array which contains the values for a specific\n",
    "                calibration coefficient for a specific date from all of\n",
    "                the calibration source files\n",
    "    Returns:\n",
    "        dataset - a list containing which calibration sources are different\n",
    "                from the other files\n",
    "        True - if all of the calibration values are the same\n",
    "        False - if the first calibration value is different\n",
    "    \"\"\"\n",
    "    # Call the function to check if there are any differences between each of\n",
    "    # calibration values from the different sheets\n",
    "    error = all_the_same(array)\n",
    "    # If they are all the same, return True\n",
    "    if all(error):\n",
    "        return True\n",
    "    # If there is a mixture of True/False, find the false and return them\n",
    "    elif any(error) == True:\n",
    "        indices = [i+1 for i, j in enumerate(error) if j == False]\n",
    "        dataset = list(array.index[indices])\n",
    "        return dataset\n",
    "    # Last, if all are false, that means the first value \n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With all the functions set up, now go through all of the data\n",
    "def search_for_errors(df):\n",
    "    \"\"\"\n",
    "    This function is designed to search through a pandas dataframe\n",
    "    which contains all of the calibration coefficients from all of\n",
    "    the files, and check for differences.\n",
    "    \n",
    "    Args: \n",
    "        df - A dataframe which contains all fo the calibration coefficients\n",
    "        from the asset management csv, qct checkout, and the vendor\n",
    "        files (.cal and .xmlcon)\n",
    "    Returns:\n",
    "        cal_errors - A nested dictionary containing the calibration timestamp, the\n",
    "        relevant calibration coefficient, and which file(s) have the\n",
    "        erroneous calibration file.\n",
    "    \"\"\"\n",
    "    \n",
    "    cal_errors = {}\n",
    "    for date in np.unique(df['Cal Date']):\n",
    "        df2 = df[df['Cal Date'] == date]\n",
    "        wrong_cals = {}\n",
    "        for column in df2.columns.values:\n",
    "            array = df2[column]\n",
    "            array.sort_index()\n",
    "            if array.dtype == 'datetime64[ns]':\n",
    "                pass\n",
    "            else:\n",
    "                error = locate_cal_error(array)\n",
    "                if error == False:\n",
    "                    wrong_cals.update({column:array.index[0]})\n",
    "                elif error == True:\n",
    "                    pass\n",
    "                else:\n",
    "                    wrong_cals.update({column:error})\n",
    "        \n",
    "        if len(wrong_cals) < 1:\n",
    "            cal_errors.update({str(date).split('T')[0]:'No Errors'})\n",
    "        else:\n",
    "            cal_errors.update({str(date).split('T')[0]:wrong_cals})\n",
    "    \n",
    "    return cal_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_errors = {}\n",
    "for uid in uids:\n",
    "    ce = search_for_errors(comparison[uid])\n",
    "    cal_errors.update({uid:ce})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(cal_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.DataFrame.from_dict({i: cal_errors[i] for i in cal_errors.keys()}, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('CTDBPP_Errors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a dataframe of the missing files\n",
    "df_missing = pd.DataFrame(index=uids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing['.CAL FILES'] = cal_missing.values()\n",
    "df_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing['.XML FILES'] = xml_missing.values()\n",
    "df_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing['.QCT FILES'] = qct_missing.values()\n",
    "df_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing.to_csv('CTDBPP_Missing_Files.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check which CTDBP-C Calibration files are not correctly named\n",
    "In order to check the calibration values, need to have the correctly named calibration csv files. We can check this by comparison of deployment dates with the CTDBPC calibration dates. This requires loading both the deployment csv and parsing all the file names, flagging the file names THAT MATCH, and then revisiting them in order to correct the name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the deployment csvs fo\n",
    "# Parse for all WHOI CG Deployment Sheets based on 'CP' or CG\n",
    "# Easier to check for non-CG \n",
    "deploy_csvs = []\n",
    "for file in os.listdir('../../GitHub/OOI-Integration/asset-management/deployment/'):\n",
    "    if file[0:2] == 'RS' or file[0:2] == 'CE':\n",
    "        pass\n",
    "    elif 'MOAS' in file:\n",
    "        pass\n",
    "    else:\n",
    "        deploy_csvs.append(file)\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Deployment History from the WHOI Asset Tracking System\n",
    "CTDBPF_Deploy = CTDBPF['Deployment History']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CTDBPF_Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the string at the newline to generate a list of deployments for each CTDBP-C\n",
    "CTDBPF_Deploy = CTDBPF['Deployment History'].apply(lambda x: x.split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CTDBPF_Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List out all the individual deployments\n",
    "deploy_list = []\n",
    "for i in range(0,len(CTDBPF_Deploy)):\n",
    "    for item in CTDBPF_Deploy.iloc[i]:\n",
    "        if '-' in item:\n",
    "            deploy_list.append(item)\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So I now have a list of the deployments all the CTDBP-Cs were used on.\n",
    "# Now, parse the name of the array to\n",
    "array = list( set( [x.split('-')[0] for x in deploy_list] ) )\n",
    "array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the list of array names, I can now parse the deployment file names to find\n",
    "# the relevant deployment sheets which match where the CTDBP-Cs were deployed\n",
    "deploy_csvs = []\n",
    "for file in os.listdir('../../GitHub/OOI-Integration/asset-management/deployment/'):\n",
    "    if file.split('_')[0] in array:\n",
    "        deploy_csvs.append(file)\n",
    "deploy_csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the identified deployment csvs, can now load the deployment csvs into\n",
    "# a pandas dataframe\n",
    "deployments = pd.DataFrame()\n",
    "for file in deploy_csvs:\n",
    "    deployments = deployments.append(pd.read_csv('../../GitHub/OOI-Integration/asset-management/deployment/'+file))\n",
    "deployments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the CTDBPF sensor uids\n",
    "sensor_uids = list( set( CTDBPF['UID'] ) )\n",
    "sensor_uids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find in the deployment spreadsheets the matching entry for the CTDBP-Cs that I'm looking for\n",
    "deployments['CTDBPF'] = deployments['sensor.uid'].apply(lambda x: True if x in sensor_uids else False)\n",
    "deployments = deployments[deployments['CTDBPF'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, parse out the date string in the format of YYYYMMDD from the startDateTime\n",
    "# in order to compare with the date in the calibration file names\n",
    "deploy_dates = deployments['startDateTime'].apply(lambda x: x.replace('-','').split('T')[0])\n",
    "deploy_dates = list(set(deploy_dates))\n",
    "deploy_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_csvs = []\n",
    "for file in os.listdir('../../GitHub/OOI-Integration/asset-management/calibration/CTDBPF/'):\n",
    "    date = file.split('__')[1].split('.')[0]\n",
    "    print(date)\n",
    "    if date in deploy_dates:\n",
    "        cal_csvs = cal_csvs.append(file)\n",
    "print(cal_csvs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_csvs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! None of the CTDBP-C have calibration dates which match deployment dates. That is a good sign - it means that the dates in the calibration file name *should* match the calibration dates in the calibration info.\n",
    "\n",
    "However, that is no guarantee that the date in the file name matches the date in the calibration data. This can be check in a future step by comparing the calibration date in the vendor docs, QCT info, and the .cal and .xmlcon file info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, using the \"deploy\" csvs for each node in the various arrays,\n",
    "# need to load into a large pandas dataframe for easy handling\n",
    "import pandas as pd\n",
    "\n",
    "deployments = pd.DataFrame()\n",
    "for file in deploy_csvs:\n",
    "    deployments = deployments.append(pd.read_csv('../GitHub/OOI-Integration/asset-management/deployment/'+file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the unique deployment dates from the deployment csvs and put into the form of \n",
    "# YYYYMMDD. \n",
    "deploy_dates = deployments['startDateTime'].apply(lambda x: x.split('T')[0].replace('-',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy_dates = list(set(deploy_dates))\n",
    "deploy_dates[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(deploy_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_files = []\n",
    "for root, dirs, files in os.walk('../GitHub/OOI-Integration/asset-management/calibration/'):\n",
    "    for name in files:\n",
    "        if 'CGINS' in name:\n",
    "            cal_date = name.split('__')[1].split('.')[0]\n",
    "            if cal_date in deploy_dates:\n",
    "                check_files.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import re\n",
    "import os\n",
    "from wcmatch import fnmatch\n",
    "import xml.etree.ElementTree as et\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from zipfile import ZipFile\n",
    "import csv\n",
    "import PyPDF2\n",
    "from nltk.tokenize import word_tokenize\n",
    "import json\n",
    "\n",
    "class CTDCalibration():\n",
    "    # Class that stores calibration values for CTDs.\n",
    "\n",
    "    def __init__(self, uid):\n",
    "        self.serial = ''\n",
    "        self.uid = uid\n",
    "        self.ctd_type = uid\n",
    "        self.coefficients = {}\n",
    "        self.date = {}\n",
    "\n",
    "        self.coefficient_name_map = {\n",
    "            'TA0': 'CC_a0',\n",
    "            'TA1': 'CC_a1',\n",
    "            'TA2': 'CC_a2',\n",
    "            'TA3': 'CC_a3',\n",
    "            'CPCOR': 'CC_cpcor',\n",
    "            'CTCOR': 'CC_ctcor',\n",
    "            'CG': 'CC_g',\n",
    "            'CH': 'CC_h',\n",
    "            'CI': 'CC_i',\n",
    "            'CJ': 'CC_j',\n",
    "            'G': 'CC_g',\n",
    "            'H': 'CC_h',\n",
    "            'I': 'CC_i',\n",
    "            'J': 'CC_j',\n",
    "            'PA0': 'CC_pa0',\n",
    "            'PA1': 'CC_pa1',\n",
    "            'PA2': 'CC_pa2',\n",
    "            'PTEMPA0': 'CC_ptempa0',\n",
    "            'PTEMPA1': 'CC_ptempa1',\n",
    "            'PTEMPA2': 'CC_ptempa2',\n",
    "            'PTCA0': 'CC_ptca0',\n",
    "            'PTCA1': 'CC_ptca1',\n",
    "            'PTCA2': 'CC_ptca2',\n",
    "            'PTCB0': 'CC_ptcb0',\n",
    "            'PTCB1': 'CC_ptcb1',\n",
    "            'PTCB2': 'CC_ptcb2',\n",
    "            # additional types for series O\n",
    "            'C1': 'CC_C1',\n",
    "            'C2': 'CC_C2',\n",
    "            'C3': 'CC_C3',\n",
    "            'D1': 'CC_D1',\n",
    "            'D2': 'CC_D2',\n",
    "            'T1': 'CC_T1',\n",
    "            'T2': 'CC_T2',\n",
    "            'T3': 'CC_T3',\n",
    "            'T4': 'CC_T4',\n",
    "            'T5': 'CC_T5',\n",
    "        }\n",
    "\n",
    "        # Name mapping for the MO-type CTDs (when reading from pdfs)\n",
    "        self.mo_coefficient_name_map = {\n",
    "            'ptcb1': 'CC_ptcb1',\n",
    "            'pa2': 'CC_pa2',\n",
    "            'a3': 'CC_a3',\n",
    "            'pa0': 'CC_pa0',\n",
    "            'wbotc': 'CC_wbotc',\n",
    "            'ptcb0': 'CC_ptcb0',\n",
    "            'g': 'CC_g',\n",
    "            'ptempa1': 'CC_ptempa1',\n",
    "            'ptcb2': 'CC_ptcb2',\n",
    "            'a0': 'CC_a0',\n",
    "            'h': 'CC_h',\n",
    "            'ptca0': 'CC_ptca0',\n",
    "            'a2': 'CC_a2',\n",
    "            'cpcor': 'CC_cpcor',\n",
    "            'i': 'CC_i',\n",
    "            'ptempa0': 'CC_ptempa0',\n",
    "            'prange': 'CC_p_range',\n",
    "            'ctcor': 'CC_ctcor',\n",
    "            'a1': 'CC_a1',\n",
    "            'j': 'CC_j',\n",
    "            'ptempa2': 'CC_ptempa2',\n",
    "            'pa1': 'CC_pa1',\n",
    "            'ptca1': 'CC_ptca1',\n",
    "            'ptca2': 'CC_ptca2',\n",
    "        }\n",
    "\n",
    "        self.o2_coefficients_map = {\n",
    "            'A': 'CC_residual_temperature_correction_factor_a',\n",
    "            'B': 'CC_residual_temperature_correction_factor_b',\n",
    "            'C': 'CC_residual_temperature_correction_factor_c',\n",
    "            'E': 'CC_residual_temperature_correction_factor_e',\n",
    "            'SOC': 'CC_oxygen_signal_slope',\n",
    "            'OFFSET': 'CC_frequency_offset'\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def uid(self):\n",
    "        return self._uid\n",
    "\n",
    "    @uid.setter\n",
    "    def uid(self, d):\n",
    "        r = re.compile('.{5}-.{6}-.{5}')\n",
    "        if r.match(d) is not None:\n",
    "            self.serial = d.split('-')[2]\n",
    "            self._uid = d\n",
    "        else:\n",
    "            raise Exception(f\"The instrument uid {d} is not a valid uid. Please check.\")\n",
    "\n",
    "    @property\n",
    "    def ctd_type(self):\n",
    "        return self._ctd_type\n",
    "\n",
    "    @ctd_type.setter\n",
    "    def ctd_type(self, d):\n",
    "        if 'MO' in d:\n",
    "            self._ctd_type = '37'\n",
    "        elif 'BP' in d:\n",
    "            self._ctd_type = '16'\n",
    "        else:\n",
    "            self._ctd_type = ''\n",
    "\n",
    "    def load_pdf(self, filepath):\n",
    "        \"\"\"\n",
    "        This function opens and loads a pdf into a parseable format.\n",
    "\n",
    "        Args:\n",
    "            filepath - full directory path with filename\n",
    "        Raises:\n",
    "            IOError - error reading or loading text from the pdf object\n",
    "        Returns:\n",
    "            text - a dictionary with page numbers as keys and the pdf text as items\n",
    "        \"\"\"\n",
    "\n",
    "        # Open and read the pdf file\n",
    "        pdfFileObj = open(filepath, 'rb')\n",
    "        # Create a reader to be parsed\n",
    "        pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "        # Now, enumerate through the pdf and decode the text\n",
    "        num_pages = pdfReader.numPages\n",
    "        count = 0\n",
    "        text = {}\n",
    "\n",
    "        while count < num_pages:\n",
    "            pageObj = pdfReader.getPage(count)\n",
    "            count = count + 1\n",
    "            text.update({count: pageObj.extractText()})\n",
    "\n",
    "        # Run a check that text was actually extracted\n",
    "        if len(text) == 0:\n",
    "            raise(IOError(f'No text was parsed from the pdf file {filepath}'))\n",
    "        else:\n",
    "            return text\n",
    "\n",
    "    def read_pdf(self, filepath):\n",
    "        \"\"\"\n",
    "        Function which parses the opened and loaded pdf file into the\n",
    "        relevant calibration coefficient data. This function works if\n",
    "        the calibration pdfs have been split based on sensor as well as\n",
    "        for combined pdfs.\n",
    "\n",
    "        Args:\n",
    "            text - the opened and loaded pdf text returned from load_pdf\n",
    "        Raises:\n",
    "            Exception - thrown when a relevant calibration information is\n",
    "                missing from the text\n",
    "        Returns:\n",
    "            date - the calibration dates of the temperature, conductivity,\n",
    "                and pressure sensors of the CTDMO in a dictionary object\n",
    "            serial - populated serial number of the CTDMO\n",
    "            coefficients - populated dictionary of the calibration coefficients\n",
    "                as keys and associated values as items.\n",
    "        \"\"\"\n",
    "        text = self.load_pdf(filepath)\n",
    "\n",
    "        for page_num in text.keys():\n",
    "            # Search for the temperature calibration data\n",
    "            if 'SBE 37 TEMPERATURE CALIBRATION DATA' in text[page_num]:\n",
    "                tokens = word_tokenize(text[page_num])\n",
    "                data = [word.lower() for word in tokens if not word in string.punctuation]\n",
    "                # Now, find and record the calibration date\n",
    "                if 'calibration' and 'date' in data:\n",
    "                    cal_ind = data.index('calibration')\n",
    "                    date_ind = data.index('date')\n",
    "                    # Run check they are in order\n",
    "                    if date_ind == cal_ind+1:\n",
    "                        date = pd.to_datetime(data[date_ind+1]).strftime('%Y%m%d')\n",
    "                        self.date.update({'TCAL':date})\n",
    "                    else:\n",
    "                        raise Exception(f\"Can't locate temp calibration date.\")\n",
    "                else:\n",
    "                    raise Exception(f\"Can't locate temp calibration date.\")\n",
    "\n",
    "                # Check for the serial number\n",
    "                if 'serial' and 'number' in data and len(self.serial) == 0:\n",
    "                    ser_ind = data.index('serial')\n",
    "                    num_ind = data.index('number')\n",
    "                    if num_ind == ser_ind+1:\n",
    "                        self.serial = data[num_ind+1]\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "                # Now, get the calibration coefficients\n",
    "                for key in self.mo_coefficient_name_map.keys():\n",
    "                    if key in data:\n",
    "                        ind = data.index(key)\n",
    "                        self.coefficients.update({self.mo_coefficient_name_map[key]: data[ind+1]})\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "            # Search for the conductivity calibration data\n",
    "            elif 'SBE 37 CONDUCTIVITY CALIBRATION DATA' in text[page_num]:\n",
    "                # tokenize the text data and extract only key words\n",
    "                tokens = word_tokenize(text[page_num])\n",
    "                data = [word.lower() for word in tokens if not word in string.punctuation]\n",
    "\n",
    "                # Now, find and record the calibration date\n",
    "                if 'calibration' and 'date' in data:\n",
    "                    cal_ind = data.index('calibration')\n",
    "                    date_ind = data.index('date')\n",
    "                    # Run check they are in order\n",
    "                    if date_ind == cal_ind+1:\n",
    "                        date = pd.to_datetime(data[date_ind+1]).strftime('%Y%m%d')\n",
    "                        self.date.update({'CCAL': date})\n",
    "                    else:\n",
    "                        raise Exception(f\"Can't locate conductivity calibration date.\")\n",
    "                else:\n",
    "                    raise Exception(f\"Can't locate conductivity calibration date.\")\n",
    "\n",
    "                # Check for the serial number\n",
    "                if 'serial' and 'number' in data and len(self.serial) == 0:\n",
    "                    ser_ind = data.index('serial')\n",
    "                    num_ind = data.index('number')\n",
    "                    if num_ind == ser_ind+1:\n",
    "                        self.serial = data[num_ind+1]\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "                # Now, get the calibration coefficients\n",
    "                for key in self.mo_coefficient_name_map.keys():\n",
    "                    if key in data:\n",
    "                        ind = data.index(key)\n",
    "                        self.coefficients.update({self.mo_coefficient_name_map[key]: data[ind+1]})\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "            elif 'SBE 37 PRESSURE CALIBRATION DATA' in text[page_num]:\n",
    "                # tokenize the text data and extract only key words\n",
    "                tokens = word_tokenize(text[page_num])\n",
    "                data = [word.lower() for word in tokens if not word in string.punctuation]\n",
    "\n",
    "                # Now, find and record the calibration date\n",
    "                if 'calibration' and 'date' in data:\n",
    "                    cal_ind = data.index('calibration')\n",
    "                    date_ind = data.index('date')\n",
    "                    # Run check they are in order\n",
    "                    if date_ind == cal_ind+1:\n",
    "                        date = pd.to_datetime(data[date_ind+1]).strftime('%Y%m%d')\n",
    "                        self.date.update({'PCAL': date})\n",
    "                    else:\n",
    "                        raise Exception(f\"Can't locate pressure calibration date.\")\n",
    "                else:\n",
    "                    raise Exception(f\"Can't locate pressure calibration date.\")\n",
    "\n",
    "                # Check for the serial number\n",
    "                if 'serial' and 'number' in data and len(self.serial) == 0:\n",
    "                    ser_ind = data.index('serial')\n",
    "                    num_ind = data.index('number')\n",
    "                    if num_ind == ser_ind+1:\n",
    "                        self.serial = data[num_ind+1]\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "                # Now, get the calibration coefficients\n",
    "                for key in self.mo_coefficient_name_map.keys():\n",
    "                    if key in data:\n",
    "                        ind = data.index(key)\n",
    "                        self.coefficients.update({self.mo_coefficient_name_map[key]: data[ind+1]})\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "            # Now check for other important information\n",
    "            else:\n",
    "                tokens = word_tokenize(text[page_num])\n",
    "                data = [word.lower() for word in tokens if not word in string.punctuation]\n",
    "\n",
    "                # Now, find the sensor rating\n",
    "                if 'sensor' and 'rating' in data:\n",
    "                    ind = data.index('rating')\n",
    "                    self.coefficients.update({self.mo_coefficient_name_map['prange']: data[ind+1]})\n",
    "\n",
    "    def read_cal(self, data):\n",
    "        \"\"\"\n",
    "        Function which reads and parses the CTDBP calibration values stored\n",
    "        in a .cal file.\n",
    "\n",
    "        Args:\n",
    "            filename - the name of the calibration (.cal) file to load. If the\n",
    "                cal file is not located in the same directory as this script, the\n",
    "                full filepath also needs to be specified.\n",
    "        Returns:\n",
    "            self.coefficients - populated coefficients dictionary\n",
    "            self.date - the calibration dates associated with the calibration values\n",
    "            self.type - the type (i.e. 16+/37-IM) of the CTD\n",
    "            self.serial - populates the 5-digit serial number of the instrument\n",
    "        \"\"\"\n",
    "\n",
    "        for line in data.splitlines():\n",
    "            key, value = line.replace(\" \", \"\").split('=')\n",
    "\n",
    "            if key == 'INSTRUMENT_TYPE':\n",
    "                if value == 'SEACATPLUS':\n",
    "                    ctd_type = '16'\n",
    "                elif value == '37SBE':\n",
    "                    ctd_type = '37'\n",
    "                else:\n",
    "                    ctd_type = ''\n",
    "                if self.ctd_type != ctd_type:\n",
    "                    raise ValueError(f'CTD type in cal file {ctd_type} does not match the UID type {self.ctd_type}')\n",
    "\n",
    "            elif key == 'SERIALNO':\n",
    "                if self.serial != value.zfill(5):\n",
    "                    raise Exception(f'Serial number {value.zfill(5)} stored in cal file does not match {self.serial} from the UID.')\n",
    "\n",
    "            elif 'CALDATE' in key:\n",
    "                self.date.update({key: datetime.datetime.strptime(value, '%d-%b-%y').strftime('%Y%m%d')})\n",
    "\n",
    "            else:\n",
    "                if self.ctd_type == '16':\n",
    "                    name = self.coefficient_name_map.get(key)\n",
    "                elif self.ctd_type == '37':\n",
    "                    name = self.mo_coefficient_name_map.get(key)\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "                if not name or name is None:\n",
    "                    continue\n",
    "                else:\n",
    "                    self.coefficients.update({name: value})\n",
    "\n",
    "    def load_cal(self, filepath):\n",
    "        \"\"\"\n",
    "        Loads all of the calibration coefficients from the vendor cal files for\n",
    "        a given CTD instrument class.\n",
    "\n",
    "        Args:\n",
    "            filepath - directory path to where the zipfiles are stored locally\n",
    "        Raises:\n",
    "            FileExistsError - Checks the given filepath that a .cal file exists\n",
    "        Returns:\n",
    "            self.coefficients - populated coefficients dictionary\n",
    "            self.date - the calibration dates associated with the calibration values\n",
    "            self.type - the type (i.e. 16+/37-IM) of the CTD\n",
    "            self.serial - populates the 5-digit serial number of the instrument\n",
    "        \"\"\"\n",
    "\n",
    "        if filepath.endswith('.zip'):\n",
    "            with ZipFile(filepath) as zfile:\n",
    "                filename = [name for name in zfile.namelist() if '.cal' in name]\n",
    "                if len(filename) > 0:\n",
    "                    data = zfile.read(filename[0]).decode('ASCII')\n",
    "                    self.read_cal(data)\n",
    "                else:\n",
    "                    FileExistsError(f\"No .cal file found in {filepath}.\")\n",
    "\n",
    "        elif filepath.endswith('.cal'):\n",
    "            with open(filepath) as filename:\n",
    "                data = filename.read()\n",
    "                self.read_cal(data)\n",
    "\n",
    "        else:\n",
    "            FileExistsError(f\"No .cal file found in {filepath}.\")\n",
    "\n",
    "    def read_xml(self, data):\n",
    "        \"\"\"\n",
    "        Function which reads and parses the CTDBP calibration values stored\n",
    "        in the xmlcon file.\n",
    "\n",
    "        Args:\n",
    "            data - the data string to parse\n",
    "        Returns:\n",
    "            self.coefficients - populated coefficients dictionary\n",
    "            self.date - the calibration dates associated with the calibration values\n",
    "            self.type - the type (i.e. 16+/37-IM) of the CTD\n",
    "            self.serial - populates the 5-digit serial number of the instrument\n",
    "        \"\"\"\n",
    "\n",
    "        Tflag  = False\n",
    "        Cflag  = False\n",
    "        O2flag = False\n",
    "\n",
    "        for child in data.iter():\n",
    "            key = child.tag.upper()\n",
    "            value = child.text.upper()\n",
    "\n",
    "            if key == 'NAME':\n",
    "                if '16PLUS' in value:\n",
    "                    ctd_type = '16'\n",
    "                    if self.ctd_type != ctd_type:\n",
    "                        raise ValueError(f'CTD type in xmlcon file {ctd_type} does not match the UID type {self.ctd_type}')\n",
    "\n",
    "            # Check if we are processing the calibration values for the temperature sensor\n",
    "            # If we already have parsed the Temp data, need to turn the flag off\n",
    "            if key == 'TEMPERATURESENSOR':\n",
    "                Tflag = True\n",
    "            elif 'SENSOR' in key and Tflag == True:\n",
    "                Tflag = False\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            # Check on if we are now parsing the conductivity data\n",
    "            if key == 'CONDUCTIVITYSENSOR':\n",
    "                Cflag = True\n",
    "            elif 'SENSOR' in key and Cflag == True:\n",
    "                Cflag = False\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            # Check if an oxygen sensor has been appended to the CTD configuration\n",
    "            if key == 'OXYGENSENSOR':\n",
    "                O2flag = True\n",
    "\n",
    "            # Check that the serial number in the xmlcon file matches the serial\n",
    "            # number from the UID\n",
    "            if key == 'SERIALNUMBER':\n",
    "                if self.serial != value.zfill(5):\n",
    "                    raise Exception(f'Serial number {value.zfill(5)} stored in xmlcon file does not match {self.serial} from the UID.')\n",
    "\n",
    "            # Parse the calibration dates of the different sensors\n",
    "            if key == 'CALIBRATIONDATE':\n",
    "                if Tflag:\n",
    "                    self.date.update({'TCALDATE':datetime.datetime.strptime(value, '%d-%b-%y').strftime('%Y%m%d')})\n",
    "                elif Cflag:\n",
    "                    self.date.update({'CCALDATE': datetime.datetime.strptime(value, '%d-%b-%y').strftime('%Y%m%d')})\n",
    "                else:\n",
    "                    self.date.update({'PCALDATE': datetime.datetime.strptime(value, '%d-%b-%y').strftime('%Y%m%d')})\n",
    "\n",
    "            # Now, we get to parse the actual calibration values, but it is necessary to make sure the\n",
    "            # key names are correct\n",
    "            if Tflag:\n",
    "                key = 'T'+key\n",
    "\n",
    "            name = self.coefficient_name_map.get(key)\n",
    "            if not name or name is None:\n",
    "                if O2flag:\n",
    "                    name = self.o2_coefficients_map.get(key)\n",
    "                    self.coefficients.update({name: value})\n",
    "                else:\n",
    "                    pass\n",
    "            else:\n",
    "                self.coefficients.update({name: value})\n",
    "\n",
    "    def load_xml(self, filepath):\n",
    "        \"\"\"\n",
    "        Loads all of the calibration coefficients from the vendor xmlcon files for\n",
    "        a given CTD instrument class.\n",
    "\n",
    "        Args:\n",
    "            filepath - the name of the xmlcon file to load and parse. If the\n",
    "                xmlcon file is not located in the same directory as this script,\n",
    "                the full filepath also needs to be specified. May point to a zipfile.\n",
    "        Raises:\n",
    "            FileExistsError - Checks the given filepath that an xmlcon file exists\n",
    "        Returns:\n",
    "            self.coefficients - populated coefficients dictionary\n",
    "            self.date - the calibration dates associated with the calibration values\n",
    "            self.type - the type (i.e. 16+/37-IM) of the CTD\n",
    "            self.serial - populates the 5-digit serial number of the instrument\n",
    "        \"\"\"\n",
    "\n",
    "        if filepath.endswith('.zip'):\n",
    "            with ZipFile(filepath) as zfile:\n",
    "                filename = [name for name in zfile.namelist() if '.xmlcon' in name]\n",
    "                if len(filename) > 0:\n",
    "                    data = et.parse(zfile.open(filename[0]))\n",
    "                    self.read_xml(data)\n",
    "                else:\n",
    "                    FileExistsError(f\"No .cal file found in {filepath}.\")\n",
    "\n",
    "        elif filepath.endswith('.xmlcon'):\n",
    "            with open(filepath) as file:\n",
    "                data = et.parse(file)\n",
    "                self.read_xml(data)\n",
    "\n",
    "        else:\n",
    "            FileExistsError(f\"No .cal file found in {filepath}.\")\n",
    "\n",
    "    def load_qct(self, filepath):\n",
    "        \"\"\"\n",
    "        Function which parses the output from the QCT check-in and loads them into\n",
    "        the CTD object.\n",
    "\n",
    "        Args:\n",
    "            filepath - the full directory path and filename\n",
    "        Raises:\n",
    "            ValueError - checks if the serial number parsed from the UID matches the\n",
    "                the serial number stored in the file.\n",
    "        Returns:\n",
    "            self.coefficients - populated coefficients dictionary\n",
    "            self.date - the calibration dates associated with the calibration values\n",
    "            self.type - the type (i.e. 16+/37-IM) of the CTD\n",
    "            self.serial - populates the 5-digit serial number of the instrument\n",
    "        \"\"\"\n",
    "\n",
    "        with open(filepath,errors='ignore') as filename:\n",
    "            data = filename.read()\n",
    "\n",
    "        if self.ctd_type == '37':\n",
    "            data = data.replace('<', ' ').replace('>', ' ')\n",
    "            for line in data.splitlines():\n",
    "                keys = list(self.mo_coefficient_name_map.keys())\n",
    "                if any([word for word in line.split() if word.lower() in keys]):\n",
    "                    name = self.mo_coefficient_name_map.get(line.split()[0])\n",
    "                    value = line.split()[-1]\n",
    "                    self.coefficients.update({name: value})\n",
    "\n",
    "        elif self.ctd_type == '16':\n",
    "            if 'CTDBPP' in self.uid:\n",
    "                # Okay, the CTDBPP (inductive mooring ones) have to be handled differently\n",
    "                data = data.replace('<', ' ').replace('>', ' ')\n",
    "                for line in data.splitlines():\n",
    "                    keys = list(self.coefficient_name_map.keys())\n",
    "                    \n",
    "                    if any([word for word in line.split() if word in keys]):\n",
    "                        name = self.coefficient_name_map.get(line.split()[0])\n",
    "                        value = line.split()[1]\n",
    "                        self.coefficients.update({name: value})\n",
    "                        \n",
    "                    elif 'SERIAL NO.' in line:\n",
    "                        ind = line.split().index('NO.')\n",
    "                        serial_num = line.split()[ind+1]\n",
    "                        serial_num = serial_num.zfill(5)\n",
    "                        if not self.serial == serial_num:\n",
    "                            raise ValueError(f'UID serial number {self.serial} does not match the QCT serial num {serial_num}')\n",
    "                    \n",
    "                    elif 'CalDate' in line:\n",
    "                        words = line.split()\n",
    "                        self.date.update({str(len(self.date)): pd.to_datetime(words[1]).strftime('%Y%m%d')})\n",
    "                    \n",
    "                    else:\n",
    "                        pass\n",
    "            \n",
    "            else:\n",
    "                for line in data.splitlines():\n",
    "                    keys = list(self.coefficient_name_map.keys())\n",
    "                    if any([word for word in line.split() if word in keys]):\n",
    "                        name = self.coefficient_name_map.get(line.split()[0])\n",
    "                        value = line.split()[-1]\n",
    "                        self.coefficients.update({name: value})\n",
    "\n",
    "                    if 'temperature:' in line:\n",
    "                        self.date.update({'TCAL': pd.to_datetime(line.split()[-1]).strftime('%Y%m%d')})\n",
    "                    elif 'conductivity:' in line:\n",
    "                        self.date.update({'CCAL': pd.to_datetime(line.split()[-1]).strftime('%Y%m%d')})\n",
    "                    elif 'pressure S/N' in line:\n",
    "                        self.date.update({'PCAL': pd.to_datetime(line.split()[-1]).strftime('%Y%m%d')})\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "                    if 'SERIAL NO.' in line:\n",
    "                        ind = line.split().index('NO.')\n",
    "                        serial_num = line.split()[ind+1]\n",
    "                        serial_num = serial_num.zfill(5)\n",
    "                        if not self.serial == serial_num:\n",
    "                            raise ValueError(f'UID serial number {self.serial} does not match the QCT serial num {serial_num}')\n",
    "\n",
    "                if 'SBE 16Plus' in line:\n",
    "                    if self.ctd_type is not '16':\n",
    "                        raise TypeError(f'CTD type {self.ctd_type} does not match the qct.')\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    def write_csv(self, outpath):\n",
    "        \"\"\"\n",
    "        This function writes the correctly named csv file for the ctd to the\n",
    "        specified directory.\n",
    "\n",
    "        Args:\n",
    "            outpath - directory path of where to write the csv file\n",
    "        Raises:\n",
    "            ValueError - raised if the CTD object's coefficient dictionary\n",
    "                has not been populated\n",
    "        Returns:\n",
    "            self.to_csv - a csv of the calibration coefficients which is\n",
    "                written to the specified directory from the outpath.\n",
    "        \"\"\"\n",
    "\n",
    "        # Run a check that the coefficients have actually been loaded\n",
    "        if len(self.coefficients) == 0:\n",
    "            raise ValueError('No calibration coefficients have been loaded.')\n",
    "\n",
    "        # Create a dataframe to write to the csv\n",
    "        data = {'serial': [self.ctd_type + '-' + self.serial]*len(self.coefficients),\n",
    "                'name': list(self.coefficients.keys()),\n",
    "                'value': list(self.coefficients.values()),\n",
    "                'notes': ['']*len(self.coefficients)\n",
    "                }\n",
    "        df = pd.DataFrame().from_dict(data)\n",
    "\n",
    "        # Generate the csv name\n",
    "        cal_date = max(self.date.values())\n",
    "        csv_name = self.uid + '__' + cal_date + '.csv'\n",
    "\n",
    "        # Write the dataframe to a csv file\n",
    "        # check = input(f\"Write {csv_name} to {outpath}? [y/n]: \")\n",
    "        check = 'y'\n",
    "        if check.lower().strip() == 'y':\n",
    "            df.to_csv(outpath+'/'+csv_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
