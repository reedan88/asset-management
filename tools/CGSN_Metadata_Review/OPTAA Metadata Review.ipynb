{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPTAA METADATA REVIEW\n",
    "\n",
    "This notebook describes the process for reviewing the calibration coefficients for the OPTAAs. The purpose is to check the calibration coefficients contained in the CSVs stored within the asset management repository on GitHub, which are the coefficients utilized by OOI-net for calculating data products, against the different available sources of calibration information to identify when errors were made during entering the calibration csvs. This includes checking the following information:\n",
    "1. The calibration date - this information is stored in the filename of the csv\n",
    "2. Calibration source - identifying all the possible sources of calibration information, and determine which file should supply the calibration info\n",
    "3. Calibration coeffs - checking the accuracy and precision of the numbers stored in the calibration coefficients\n",
    "4. Calibration .ext files - arrays which are referenced by the main csv files and contain more calibration values\n",
    "\n",
    "The OPTAAs contains 8 different calibration coefficients to check. Five of the coefficients are arrays of varying lengths of values. Additionally, there are two .ext files which are referenced by the calibration csv. These .ext files are separate arrays of values whose name and values also need to be checked. The possible calibration source for the OPTAAs are vendor calibration (.dev) files. The QCT checkin, pre- and post-deployment files do not contain all the necessary calibration information in order to fully check the asset management csvs.\n",
    "\n",
    "**========================================================================================================================**\n",
    "\n",
    "The first step is to load relevant packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=======================================================================================================================**\n",
    "Define the directories where the QCT, Pre, and Post deployment document files are stored, where the vendor documents are stored, where asset tracking is located, and where the calibration csvs are located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_directory = '/media/andrew/OS/Users/areed/Documents/Project_Files/Records/Instrument_Records/OPTAA/OPTAA_Results/'\n",
    "cal_directory = '/media/andrew/OS/Users/areed/Documents/Project_Files/Records/Instrument_Records/OPTAA/OPTAA_Cal/'\n",
    "asset_management_directory = '/home/andrew/Documents/OOI-CGSN/ooi-integration/asset-management/calibration/OPTAAD/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_spreadsheet = '/media/andrew/OS/Users/areed/Documents/Project_Files/Documentation/System/System Notebook/WHOI_Asset_Tracking.xlsx'\n",
    "sheet_name = 'Sensors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTAA = whoi_asset_tracking(spreadsheet=excel_spreadsheet,sheet_name=sheet_name,instrument_class='OPTAA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=======================================================================================================================**\n",
    "Now, I want to load all the calibration csvs and group them by UID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uids = sorted( list( set(OPTAA['UID']) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_dict = {}\n",
    "asset_management = os.listdir(asset_management_directory)\n",
    "for uid in uids:\n",
    "    files = [file for file in asset_management if uid in file]\n",
    "    csv_dict.update({uid: sorted(files)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_dict;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=======================================================================================================================**\n",
    "Get the serial numbers of the instruments and match them to the UIDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serial_dict = {}\n",
    "for uid in uids:\n",
    "    sn = OPTAA[OPTAA['UID'] == uid]['Supplier\\nSerial Number']\n",
    "    serial_dict.update({uid: str(sn.iloc[0])})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serial_dict;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=======================================================================================================================**\n",
    "The OPTAA QCT capture files are stored with the following Document Control Numbers (DCNs): 3305-00113-XXXXX. Most are storead as **.dat** files which are easy to parse and decode (same formatting as the **.dev** files). However, some are stored as Excel (**.xlsx**) files, which are much trickier to parse.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [file for file in os.listdir(doc_directory) if 'A' in file or 'B' in file]\n",
    "qct_files = []\n",
    "for file in files:\n",
    "    if '113' in file:\n",
    "        qct_files.append(file)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qct_dict = {}\n",
    "for uid in uids:\n",
    "    # Get the QCT Document numbers from the asset tracking sheet\n",
    "    OPTAA['UID_match'] = OPTAA['UID'].apply(lambda x: True if uid in x else False)\n",
    "    qct_series = OPTAA[OPTAA['UID_match'] == True]['QCT Testing']\n",
    "    qct_series = list(qct_series.iloc[0].split('\\n'))\n",
    "    qct_dict.update({uid:qct_series})\n",
    "qct_paths = {}\n",
    "for uid in sorted(qct_dict.keys()):\n",
    "    paths = []\n",
    "    for file in qct_dict.get(uid):\n",
    "        path = generate_file_path(doc_directory, file, ext=['.dat','.xlsx'])\n",
    "        paths.append(path)\n",
    "    qct_paths.update({uid: paths})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qct_paths;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=======================================================================================================================**\n",
    "Get the pre-deployment capture files which have the following DCN: 3305-00313-XXXXX. However, the OPTAA Predeployment procedure does not involve capturing any calibration information. Thus, we do not have any relevant calibration values to test the calibration csvs against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_paths = {}\n",
    "for uid in sorted(csv_dict.keys()):\n",
    "    paths = []\n",
    "    for file in csv_dict.get(uid):\n",
    "        path = generate_file_path(asset_management_directory, file, ext=['.csv','.ext'])\n",
    "        paths.append(path)\n",
    "    csv_paths.update({uid: paths})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_paths;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=======================================================================================================================** Find and return the calibration files which contain vendor supplied calibration information. This is achieved by searching the calibration directories and matching serial numbers to UIDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serial_nums = get_serial_nums(OPTAA, uids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_dict = get_calibration_files(serial_nums, cal_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_paths = {}\n",
    "for uid in sorted(cal_dict.keys()):\n",
    "    paths = []\n",
    "    for file in cal_dict.get(uid):\n",
    "        path = generate_file_path(cal_directory, file, ext=['.zip','.cap', '.txt', '.log'])\n",
    "        paths.append(path)\n",
    "    cal_paths.update({uid: paths})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_paths;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=======================================================================================================================**\n",
    "# Parsing Calibration Coefficients\n",
    "Above, we have worked through identifying and mapping the calibration files, pre-deployment files, and post-deployment files to the individual instruments through their UIDs and serial numbers. The next step is to open the relevant files and parse out the calibration coefficients. This will require writing a parser for the NUTNRs, including sub-functions to handle the different characteristics of the ISUS and SUNA instruments.\n",
    "\n",
    "Start by opening the calibration files and read the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def from_excel_ordinal(ordinal, _epoch0=datetime(1899, 12, 31)):\n",
    "    if ordinal > 59:\n",
    "        ordinal -= 1  # Excel leap year bug, 1900 is not a leap year!\n",
    "    return (_epoch0 + timedelta(days=ordinal)).replace(microsecond=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OPTAACalibration():\n",
    "    # Class that stores calibration values for CTDs.\n",
    "\n",
    "    def __init__(self, uid):\n",
    "        self.serial = None\n",
    "        self.nbins = None\n",
    "        self.uid = uid\n",
    "        self.sigfig = 6\n",
    "        self.date = []\n",
    "        self.coefficients = {\n",
    "            'CC_acwo': [],\n",
    "            'CC_awlngth': [],\n",
    "            'CC_ccwo': [],\n",
    "            'CC_cwlngth': [],\n",
    "            'CC_taarray': 'SheetRef:CC_taarray',\n",
    "            'CC_tbins': [],\n",
    "            'CC_tcal': [],\n",
    "            'CC_tcarray': 'SheetRef:CC_tcarray'\n",
    "        }\n",
    "        self.tcarray = []\n",
    "        self.taarray = []\n",
    "        self.notes = {\n",
    "            'CC_acwo': '',\n",
    "            'CC_awlngth': '',\n",
    "            'CC_ccwo': '',\n",
    "            'CC_cwlngth': '',\n",
    "            'CC_taarray': '',\n",
    "            'CC_tbins': '',\n",
    "            'CC_tcal': '',\n",
    "            'CC_taarray': ''\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def uid(self):\n",
    "        return self._uid\n",
    "\n",
    "    @uid.setter\n",
    "    def uid(self, d):\n",
    "        r = re.compile('.{5}-.{6}-.{5}')\n",
    "        if r.match(d) is not None:\n",
    "            self._uid = d\n",
    "            serial = d.split('-')[-1].lstrip('0')\n",
    "            self.serial = 'ACS-' + serial\n",
    "        else:\n",
    "            raise Exception(f\"The instrument uid {d} is not a valid uid. Please check.\")\n",
    "\n",
    "            \n",
    "    def load_cal(self, filepath):\n",
    "        \"\"\"\n",
    "        Wrapper function to load all of the calibration coefficients\n",
    "        \n",
    "        Args:\n",
    "            filepath - path to the directory with filename which has the\n",
    "                calibration coefficients to be parsed and loaded\n",
    "        Calls:\n",
    "            open_cal\n",
    "            parse_cal\n",
    "        \"\"\"\n",
    "        \n",
    "        data = self.open_dev(filepath)\n",
    "        \n",
    "        self.parse_dev(data)\n",
    "        \n",
    "        \n",
    "    def load_qct(self, filepath):\n",
    "        \"\"\"\n",
    "        Wrapper function to load the calibration coefficients from\n",
    "        the QCT checkin.\n",
    "        \"\"\"\n",
    "        \n",
    "        data = self.open_dev(filepath)\n",
    "        \n",
    "        self.parse_qct(data)\n",
    "    \n",
    "    \n",
    "    def open_dev(self, filepath):\n",
    "        \"\"\"\n",
    "        Function that opens and reads in cal file\n",
    "        information for a OPTAA. Zipfiles are acceptable inputs.\n",
    "        \"\"\"\n",
    "        \n",
    "        if filepath.endswith('.zip'):\n",
    "            with ZipFile(filepath) as zfile:\n",
    "                # Check if OPTAA has the .dev file\n",
    "                filename = [name for name in zfile.namelist() if name.lower().endswith('.dev')]\n",
    "                \n",
    "                # Get and open the latest calibration file\n",
    "                if len(filename) == 1:\n",
    "                    data = zfile.read(filename[0]).decode('ascii')\n",
    "                    self.source_file(filepath, filename[0])\n",
    "                    \n",
    "                elif len(filename) > 1:\n",
    "                    raise FileExistsError(f\"Multiple .dev files found in {filepath}.\")\n",
    "\n",
    "                else:\n",
    "                    raise FileNotFoundError(f\"No .dev file found in {filepath}.\")\n",
    "                        \n",
    "        elif filepath.lower().endswith('.dev'):\n",
    "            with open(filepath) as file:\n",
    "                data = file.read()\n",
    "            self.source_file(filepath, file)\n",
    "                \n",
    "        elif filepath.lower().endswith('.dat'):\n",
    "            with open(filepath) as file:\n",
    "                data = file.read()\n",
    "            self.source_file(filepath, file)\n",
    "            \n",
    "        else:\n",
    "            raise FileNotFoundError(f\"No .dev file found in {filepath}.\")\n",
    "        \n",
    "        return data\n",
    "\n",
    "\n",
    "    def source_file(self, filepath, filename):\n",
    "        \"\"\"\n",
    "        Routine which parses out the source file and filename\n",
    "        where the calibration coefficients are sourced from.\n",
    "        \"\"\"\n",
    "        \n",
    "        if filepath.lower().endswith('.dev'):\n",
    "            dcn = filepath.split('/')[-2]\n",
    "            filename = filepath.split('/')[-1]\n",
    "        else:\n",
    "            dcn = filepath.split('/')[-1]\n",
    "        \n",
    "        self.source = f'Source file: {dcn} > {filename}'\n",
    "        \n",
    "\n",
    "    def parse_dev(self, data):\n",
    "        \"\"\"\n",
    "        Function to parse the .dev file in order to load the\n",
    "        calibration coefficients for the OPTAA.\n",
    "        \n",
    "        Args:\n",
    "            data - opened .dev file in ascii-format\n",
    "        \"\"\"\n",
    "        \n",
    "        for line in data.splitlines():\n",
    "            # Split the data based on data -> header split\n",
    "            parts = line.split(';')\n",
    "                # If the len isn't number 2, \n",
    "            if len(parts) is not 2:\n",
    "                # Find the calibration temperature and date\n",
    "                if 'tcal' in line.lower():\n",
    "                    line = ''.join((x for x in line if x not in [y for y in string.punctuation if y is not '/']))\n",
    "                    parts = line.split()\n",
    "                    # Calibration temperature\n",
    "                    tcal = parts[1].replace('C','')\n",
    "                    tcal = float(tcal)/10\n",
    "                    self.coefficients['CC_tcal'] = tcal\n",
    "                    # Calibration date\n",
    "                    date = parts[-1].strip(string.punctuation)\n",
    "                    self.date = pd.to_datetime(date).strftime('%Y%m%d')\n",
    "        \n",
    "            else:\n",
    "                info, comment = parts\n",
    "                \n",
    "                if comment.strip().startswith('temperature bins'):\n",
    "                    tbins = [float(x) for x in info.split()]\n",
    "                    self.coefficients['CC_tbins'] = tbins\n",
    "                    \n",
    "                elif comment.strip().startswith('number'):\n",
    "                    self.nbins = int(float(info.strip()))\n",
    "                    \n",
    "                elif comment.strip().startswith('C'):\n",
    "                    if self.nbins is None:\n",
    "                        raise AttributeError(f'Failed to load number of temperature bins.')\n",
    "                        \n",
    "                    # Parse out the different calibration coefficients\n",
    "                    parts = info.split()\n",
    "                    cwlngth = float(parts[0][1:])\n",
    "                    awlngth = float(parts[1][1:])\n",
    "                    ccwo = float(parts[3])\n",
    "                    acwo = float(parts[4])\n",
    "                    tcrow = [float(x) for x in parts[5:self.nbins+5]]\n",
    "                    acrow = [float(x) for x in parts[self.nbins+5:2*self.nbins+5]]\n",
    "                \n",
    "                    # Now put the coefficients into the coefficients dictionary\n",
    "                    self.coefficients['CC_acwo'].append(acwo)\n",
    "                    self.coefficients['CC_awlngth'].append(awlngth)\n",
    "                    self.coefficients['CC_ccwo'].append(ccwo)\n",
    "                    self.coefficients['CC_cwlngth'].append(cwlngth)\n",
    "                    self.tcarray.append(tcrow)\n",
    "                    self.taarray.append(acrow)\n",
    "                    \n",
    "                    \n",
    "    def parse_qct(self, data):\n",
    "        \"\"\"\n",
    "        This function is designed to parse the QCT file, which contains the\n",
    "        calibration data in slightly different format than the .dev file\n",
    "        \"\"\"\n",
    "        \n",
    "        for line in data.splitlines():\n",
    "            if 'WetView' in line:\n",
    "                _, _, _, date, time = line.split()\n",
    "                try:\n",
    "                    date_time = date + ' ' + time\n",
    "                    self.date = pd.to_datetime(date_time).strftime('%Y%m%d')\n",
    "                except:\n",
    "                    date_time = from_excel_ordinal(float(date) + float(time))\n",
    "                    self.date = pd.to_datetime(date_time).strftime('%Y%m%d')\n",
    "                continue\n",
    "                \n",
    "            parts = line.split(';')\n",
    "            \n",
    "            if len(parts) == 2:\n",
    "                if comment.strip().startswith('temperature bins'):\n",
    "                    tbins = [float(x) for x in info.split()]\n",
    "                    self.coefficients['CC_tbins'] = tbins\n",
    "                    \n",
    "                elif comment.strip().startswith('number'):\n",
    "                    self.nbins = int(float(info.strip()))\n",
    "                    \n",
    "                elif comment.strip().startswith('C'):\n",
    "                    if self.nbins is None:\n",
    "                        raise AttributeError(f'Failed to load number of temperature bins.')\n",
    "                    # Parse out the different calibration coefficients\n",
    "                    parts = info.split()\n",
    "                    cwlngth = float(parts[0][1:])\n",
    "                    awlngth = float(parts[1][1:])\n",
    "                    ccwo = float(parts[3])\n",
    "                    acwo = float(parts[4])\n",
    "                    tcrow = [float(x) for x in parts[5:self.nbins+5]]\n",
    "                    acrow = [float(x) for x in parts[self.nbins+5:(2*self.nbins)+5]]\n",
    "                    \n",
    "                    # Now put the coefficients into the coefficients dictionary\n",
    "                    self.coefficients['CC_acwo'].append(acwo)\n",
    "                    self.coefficients['CC_awlngth'].append(awlngth)\n",
    "                    self.coefficients['CC_ccwo'].append(ccwo)\n",
    "                    self.coefficients['CC_cwlngth'].append(cwlngth)\n",
    "                    self.tcarray.append(tcrow)\n",
    "                    self.taarray.append(acrow)                \n",
    "    \n",
    "                        \n",
    "    def write_csv(self, outpath):\n",
    "        \"\"\"\n",
    "        This function writes the correctly named csv file for the ctd to the\n",
    "        specified directory.\n",
    "\n",
    "        Args:\n",
    "            outpath - directory path of where to write the csv file\n",
    "        Raises:\n",
    "            ValueError - raised if the CTD object's coefficient dictionary\n",
    "                has not been populated\n",
    "        Returns:\n",
    "            self.to_csv - a csv of the calibration coefficients which is\n",
    "                written to the specified directory from the outpath.\n",
    "        \"\"\"\n",
    "\n",
    "        # Run a check that the coefficients have actually been loaded\n",
    "        if len(self.coefficients.values()) <= 2:\n",
    "            raise ValueError('No calibration coefficients have been loaded.')\n",
    "\n",
    "        # Create a dataframe to write to the csv\n",
    "        data = {\n",
    "            'serial': [self.serial]*len(self.coefficients),\n",
    "            'name': list(self.coefficients.keys()),\n",
    "            'value': list(self.coefficients.values())\n",
    "        }\n",
    "        df = pd.DataFrame().from_dict(data)\n",
    "      \n",
    "        # Now merge the coefficients dataframe with the notes\n",
    "        notes = pd.DataFrame().from_dict({\n",
    "            'name':list(self.notes.keys()),\n",
    "            'notes':list(self.notes.values())\n",
    "        })\n",
    "        df = df.merge(notes, how='outer', left_on='name', right_on='name')\n",
    "            \n",
    "        # Add in the source file\n",
    "        df['notes'].iloc[0] = df['notes'].iloc[0] + ' ' + self.source\n",
    "        \n",
    "        # Sort the data by the coefficient name\n",
    "        df = df.sort_values(by='name')\n",
    "\n",
    "        # Generate the csv names\n",
    "        csv_name = self.uid + '__' + self.date + '.csv'\n",
    "        tca_name = self.uid + '__' + self.date + '__' + 'CC_tcarray.ext'\n",
    "        taa_name = self.uid + '__' + self.date + '__' + 'CC_taarray.ext'\n",
    "        \n",
    "        def write_array(filename, cal_array):\n",
    "            with open(filename, 'w') as out:\n",
    "                array_writer = csv.writer(out)\n",
    "                array_writer.writerows(cal_array)\n",
    "\n",
    "        # Write the dataframe to a csv file\n",
    "        check = input(f\"Write {csv_name} to {outpath}? [y/n]: \")\n",
    "        # check = 'y'\n",
    "        if check.lower().strip() == 'y':\n",
    "            df.to_csv(outpath+'/'+csv_name, index=False)\n",
    "            write_array(outpath+'/'+tca_name, self.tcarray)\n",
    "            write_array(outpath+'/'+taa_name, self.taarray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=======================================================================================================================**\n",
    "# Source Loading of Calibration Coefficients\n",
    "With an OPTAA Calibration object created, we can now begin parsing the different calibration sources for each OPTAA. We will then compare all of the calibration values from each of the sources, checking for any discrepancies between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, I plan on going through each of the OPTAA UIDs, and parse the data into csvs. For sources which contain multiple sources, I plan on extracting each of the calibrations to a temporary folder using the following structure:\n",
    "\n",
    "    <local working directory>/<temp>/<source>/data/<calibration file>\n",
    "    \n",
    "The separate calibrations will be saved using the standard UFrame naming convention with the following directory structure:\n",
    "\n",
    "    <local working directory>/<temp>/<source>/<calibration csv>\n",
    "    \n",
    "The csvs themselves will also be copied to the temporary folder. This allows for the program to be looking into the same temp directory for every NUTNR check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid = uids[26]\n",
    "uid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_directory = '/'.join((os.getcwd(),'temp'))\n",
    "# Check if the temp directory exists; if it already does, purge and rewrite\n",
    "if os.path.exists(temp_directory):\n",
    "    shutil.rmtree(temp_directory)\n",
    "    ensure_dir(temp_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the existing csvs from asset management to the temp directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in csv_paths[uid]:\n",
    "    savedir = '/'.join((temp_directory,'csv'))\n",
    "    ensure_dir(savedir)\n",
    "    savepath = '/'.join((savedir, path.split('/')[-1]))\n",
    "    shutil.copyfile(path, savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(temp_directory+'/csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=======================================================================================================================**\n",
    "Load the calibration coefficients from the vendor calibration source files. Start by extracting or copying them to the source data folder in the temporary directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_paths[uid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the calibration zip files to the local temp directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in cal_paths[uid]:\n",
    "    with ZipFile(path) as zfile:\n",
    "        files = [name for name in zfile.namelist() if name.lower().endswith('.dev')]\n",
    "        for file in files:\n",
    "            exdir = path.split('/')[-1].strip('.zip')\n",
    "            expath = '/'.join((temp_directory,'cal','data',exdir))\n",
    "            ensure_dir(expath)\n",
    "            zfile.extract(file,path=expath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the vendor calibration files to csvs following the UFrame convention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savedir = '/'.join((temp_directory,'cal'))\n",
    "ensure_dir(savedir)\n",
    "# Now parse the calibration coefficients\n",
    "for dirpath, dirnames, filenames in os.walk('/'.join((temp_directory,'cal','data'))):\n",
    "    for file in filenames:\n",
    "        filepath = os.path.join(dirpath, file)\n",
    "        # With the filepath for the given calibration retrived, I can now start an instance of the NUTNR Calibration\n",
    "        # object and begin parsing the coefficients\n",
    "        optaa = OPTAACalibration(uid)\n",
    "        optaa.load_cal(filepath)\n",
    "        optaa.write_csv(savedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=======================================================================================================================**\n",
    "Load the QCT checkin for comparison with the calibration source files. Start by extracting or copying them to the source data folder in the temporary directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qct_paths[uid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For QCT documents which were saved as excel documents (**.xlsx**) files, I need to rewrite them to the local temp data directory instead as tab-delimited csv files rather than in excel workbook format. The function below handles the conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlrd\n",
    "import csv\n",
    "\n",
    "def csv_from_excel(excelpath, csvpath):\n",
    "\n",
    "    if not excelpath.endswith('.xlsx'):\n",
    "        raise FileExistsError(\"Must be an excel workbook.\")\n",
    "        \n",
    "    wb = xlrd.open_workbook(excelpath)\n",
    "    sh = wb.sheet_by_index(0)\n",
    "    csv_file = open(csvpath, 'w')\n",
    "    wr = csv.writer(csv_file, delimiter='\\t')\n",
    "\n",
    "    for rownum in range(sh.nrows):\n",
    "        wr.writerow(sh.row_values(rownum))\n",
    "\n",
    "    csv_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the above conversion function for excel to csv, we can iterate through the QCT documents for each OPTAA, and either copy them if in the **.dat** format or convert if in **.xlsx** format. Also note that the QCT documents have the following features:\n",
    "1. No calibration temperature in the header\n",
    "2. Acquisition data after the calibration data matrix. \n",
    "\n",
    "This will require writing a separate parser for the QCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in qct_paths[uid]:\n",
    "    savedir = '/'.join((temp_directory,'qct','data'))\n",
    "    ensure_dir(savedir)\n",
    "    if path.endswith('.xlsx'):\n",
    "        filename = path.split('/')[-1].replace('xlsx','dat')\n",
    "        savepath = savedir + '/' + filename\n",
    "        csv_from_excel(path, savepath)\n",
    "    else:\n",
    "        shutil.copy(path, savedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(temp_directory+'/qct/data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the QCT calibration files to csvs following the UFrame convention:\n",
    "\n",
    "However, the QCT checkin does not contain the necessary information to produce the requisite calibration file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savedir = '/'.join((temp_directory,'qct'))\n",
    "ensure_dir(savedir)\n",
    "# Now parse the calibration coefficients\n",
    "for dirpath, dirnames, filenames in os.walk('/'.join((temp_directory,'qct','data'))):\n",
    "    for file in filenames:\n",
    "        filepath = os.path.join(dirpath, file)\n",
    "        # With the filepath for the given calibration retrived, I can now start an instance of the NUTNR Calibration\n",
    "        # object and begin parsing the coefficients\n",
    "        optaa = OPTAACalibration(uid)\n",
    "        optaa.load_qct(filepath)\n",
    "        optaa.write_csv(savedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=======================================================================================================================**\n",
    "# Calibration Coefficient Comparison\n",
    "We have now successfully parsed the calibration files from all the possible sources: the vendor calibration files, the pre-deployments files, and the post-deployment files. Furthermore, we have saved csvs in the UFrame format for all of these calibrations. Now, we want to load those csvs into pandas dataframes, which allow for easy element-by-element comparison of calibration coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_date(x):\n",
    "    x = str(x)\n",
    "    ind1 = x.index('__')\n",
    "    ind2 = x.index('.')\n",
    "    return x[ind1+2:ind2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to compare dataframe\n",
    "csv_files = pd.DataFrame(sorted(csv_dict[uid]),columns=['csv'])\n",
    "csv_files['cal date'] = csv_files['csv'].apply(lambda x: get_file_date(x))\n",
    "csv_files.set_index('cal date',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to compare dataframe\n",
    "cal_files = pd.DataFrame(sorted(os.listdir('temp/cal')),columns=['cal'])\n",
    "cal_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iloc = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_files.drop([iloc],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_files['cal date'] = cal_files['cal'].apply(lambda x: get_file_date(x))\n",
    "cal_files.set_index('cal date',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_files = csv_files.join(cal_files,how='outer').fillna(value='-999')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to compare the dates in the CSV and QCT files against the **.dev** CAL files, which contain the date that the OPTAA itself was calibrated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSV files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn = '00257'\n",
    "d1 = '20161011'\n",
    "d2 = '20160826'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = 'temp/csv/' + f'CGINS-OPTAAD-{sn}__{d1}.csv'\n",
    "dst = 'temp/csv/' + f'CGINS-OPTAAD-{sn}__{d2}.csv'\n",
    "shutil.move(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = 'temp/csv/' + f'CGINS-OPTAAD-{sn}__{d1}__CC_taarray.ext'\n",
    "dst = 'temp/csv/' + f'CGINS-OPTAAD-{sn}__{d2}__CC_taarray.ext'\n",
    "shutil.move(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = 'temp/csv/' + f'CGINS-OPTAAD-{sn}__{d1}__CC_tcarray.ext'\n",
    "dst = 'temp/csv/' + f'CGINS-OPTAAD-{sn}__{d2}__CC_tcarray.ext'\n",
    "shutil.move(src, dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload the csv files in order to perform the comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to compare dataframe\n",
    "csv_files = pd.DataFrame(sorted(os.listdir('temp/csv')),columns=['csv'])\n",
    "csv_files['cal date'] = csv_files['csv'].apply(lambda x: get_file_date(x))\n",
    "csv_files.set_index('cal date',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to compare dataframe\n",
    "cal_files = pd.DataFrame(sorted(os.listdir('temp/cal')),columns=['cal'])\n",
    "cal_files.drop([iloc],inplace=True)\n",
    "cal_files['cal date'] = cal_files['cal'].apply(lambda x: get_file_date(x))\n",
    "cal_files.set_index('cal date',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_files = csv_files.join(cal_files,how='outer').fillna(value='-999')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=======================================================================================================================**\n",
    "Now, with the csv files correctly named, we can load the info into pandas dataframe which will allow for the direct comparison of calibration coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = '20160826'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = f'CGINS-OPTAAD-{sn}__{dt}.csv'\n",
    "b = f'CGINS-OPTAAD-{sn}__{dt}__CC_taarray.ext'\n",
    "c = f'CGINS-OPTAAD-{sn}__{dt}__CC_tcarray.ext'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV = pd.read_csv('temp/csv/'+a)\n",
    "with open('temp/csv/'+b) as file:\n",
    "    csv_ta = file.read()\n",
    "    CSV_ta = []\n",
    "    for line in csv_ta.splitlines():\n",
    "        line = [float(x) for x in line.split(',')]\n",
    "        CSV_ta.append(line)\n",
    "with open('temp/csv/'+c) as file:\n",
    "    csv_tc = file.read()\n",
    "    CSV_tc = []\n",
    "    for line in csv_tc.splitlines():\n",
    "        line = line.replace('[','').replace(']','')\n",
    "        line = [float(x) for x in line.split(',')]\n",
    "        CSV_tc.append(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEV = pd.read_csv('temp/cal/'+a)\n",
    "with open('temp/cal/'+b) as file:\n",
    "    dev_ta = file.read()\n",
    "    DEV_ta = []\n",
    "    for line in dev_ta.splitlines():\n",
    "        line = [float(x) for x in line.split(',')]\n",
    "        DEV_ta.append(line)\n",
    "with open('temp/cal/'+c) as file:\n",
    "    dev_tc = file.read()\n",
    "    DEV_tc = []\n",
    "    for line in dev_tc.splitlines():\n",
    "        line = [float(x) for x in line.split(',')]\n",
    "        DEV_tc.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_arrays(array):\n",
    "    # First, need to strip extraneous characters from the array\n",
    "    array = array.replace(\"'\",\"\").replace('[','').replace(']','')\n",
    "    # Next, split the array into a list\n",
    "    array = array.split(',')\n",
    "    # Now, need to eliminate any white space surrounding the individual coeffs\n",
    "    array = [num.strip() for num in array]\n",
    "    # Next, float the nums\n",
    "    try:\n",
    "        array = [float(num) for num in array]\n",
    "        # Check if the array is len == 1; if so, can just return the number\n",
    "        if len(array) == 1:\n",
    "            array = array[0]\n",
    "    except:\n",
    "        pass\n",
    "    # Now we are done\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV['value'] = CSV['value'].apply(lambda x: reformat_arrays(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEV['value'] = DEV['value'].apply(lambda x: reformat_arrays(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compare the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV = CSV.sort_values(by='name').reset_index().drop(columns='index')\n",
    "CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEV['notes'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.equal(DEV,CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.all(np.equal(DEV_ta,CSV_ta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j,k in enumerate(DEV_ta):\n",
    "    check = DEV_ta[j] == CSV_ta[j]\n",
    "    if not check:\n",
    "        for m,n in enumerate(DEV_ta[j]):\n",
    "            check2 = np.equal(DEV_ta[j][m],CSV_ta[j][m])\n",
    "            if not check2:\n",
    "                print(str(j)+','+str(m)+': ' + 'DEV - '+str(DEV_ta[j][m]) + ' CSV - '+str(CSV_ta[j][m]) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.all(np.equal(DEV_tc,CSV_tc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j,k in enumerate(DEV_tc):\n",
    "    check = DEV_tc[j] == CSV_tc[j]\n",
    "    if not check:\n",
    "        for m,n in enumerate(DEV_tc[j]):\n",
    "            check2 = DEV_tc[j][m] == CSV_tc[j][m]\n",
    "            if not check2:\n",
    "                print(str(j)+','+str(m)+': ' + 'DEV - '+str(DEV_tc[j][m]) + ' CSV - '+str(CSV_tc[j][m]) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEV_tc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optaa.nbins+5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optaa.nbins"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
