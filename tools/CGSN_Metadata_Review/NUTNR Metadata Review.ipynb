{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NUTNR METADATA REVIEW\n",
    "\n",
    "This notebook describes the process for reviewing the calibration coefficients for the NUTNRs, including both the ISUS and SUNA models. The purpose is to check the calibration coefficients contained in the CSVs stored within the asset management repository on GitHub, which are the coefficients utilized by OOI-net for calculating data products, against the different available sources of calibration information to identify when errors were made during entering the calibration csvs. This includes checking the following information:\n",
    "1. The calibration date - this information is stored in the filename of the csv\n",
    "2. Calibration source - identifying all the possible sources of calibration information, and determine which file should supply the calibration info\n",
    "3. Calibration coeffs - checking the accuracy and precision of the numbers stored in the calibration coefficients\n",
    "\n",
    "The NUTNRs contains 7 different calibration coefficients to check. Two of the calibration coefficients are fixed constants. Four of the coefficients are arrays of 35 values. The possible calibration sources for the NUTNRs are vendor calibration (.cal) files, as well as pre- and post-deployment calibrations (.cal files). A complication is that the calibration documents often contain multiple .cal files. However, if there are multiple .cal files, they are sequentially appended with the alphabet. Consequently, we identify the latest .cal file based on the appended letter to the file.\n",
    "\n",
    "**========================================================================================================================**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import likely important packages, etc.\n",
    "import sys, os, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**====================================================================================================================**\n",
    "Define the directories where the QCT, Pre, and Post deployment document files are stored, where the vendor documents are stored, where asset tracking is located, and where the calibration csvs are located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_directory = '/media/andrew/OS/Users/areed/Documents/Project_Files/Records/Instrument_Records/NUTNR/NUTNR_Results/'\n",
    "cal_directory = '/media/andrew/OS/Users/areed/Documents/Project_Files/Records/Instrument_Records/NUTNR/NUTNR_Cal/'\n",
    "asset_management_directory = '/home/andrew/Documents/OOI-CGSN/ooi-integration/asset-management/calibration/NUTNRB/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_spreadsheet = '/media/andrew/OS/Users/areed/Documents/Project_Files/Documentation/System/System Notebook/WHOI_Asset_Tracking.xlsx'\n",
    "sheet_name = 'Sensors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUTNR = whoi_asset_tracking(spreadsheet=excel_spreadsheet,sheet_name=sheet_name,instrument_class='NUTNR',series='B')\n",
    "NUTNR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**======================================================================================================================**\n",
    "Now, I want to load all the calibration csvs and group them by UID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uids = sorted( list( set(NUTNR['UID']) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_dict = {}\n",
    "asset_management = os.listdir(asset_management_directory)\n",
    "for uid in uids:\n",
    "    files = [file for file in asset_management if uid in file]\n",
    "    csv_dict.update({uid: sorted(files)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=======================================================================================================================**\n",
    "Get the serial numbers of the instruments and match them to the UIDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serial_dict = {}\n",
    "for uid in uids:\n",
    "    sn = NUTNR[NUTNR['UID'] == uid]['Supplier\\nSerial Number']\n",
    "    serial_dict.update({uid: str(sn.iloc[0])})    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=======================================================================================================================**\n",
    "Get the QCT capture files with the following Document Control Numbers (DCNs):\n",
    "* ISUS: 3305-00108-XXXXX-A\n",
    "* SUNA: 3305-00127-XXXXX-A\n",
    "\n",
    "For the NUTNRs, the QCT files do not contain any calibration information. Rather, the calibration information is contained in separate **.CAL** files, which are updated each time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [file for file in os.listdir(doc_directory) if 'A' in file]\n",
    "qct_files = []\n",
    "for file in files:\n",
    "    if '108' in file or '127' in file:\n",
    "        qct_files.append(file)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=======================================================================================================================**\n",
    "Get the pre-deployment capture files, which should contain **.CAL** files, with the following DCNs:\n",
    "* ISUS: 3305-00308-XXXXX-A\n",
    "* SUNA: 3305-00327-XXXXX-A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [file for file in os.listdir(doc_directory) if 'A' in file]\n",
    "pre_files = []\n",
    "for file in files:\n",
    "    if '308' in file or '327' in file:\n",
    "        pre_files.append(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the Pre-deployment files and get the instrument serial number to match the Pre-deployment DCN to an individual insturment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_paths = []\n",
    "predeployment = {}\n",
    "for file in pre_files:\n",
    "    path = generate_file_path(doc_directory, file, ext=['.zip'])\n",
    "    with ZipFile(path) as zfile:\n",
    "        cal_files = [file for file in zfile.namelist() if file.lower().endswith('.cal')]\n",
    "        if len(cal_files) > 0:\n",
    "            data = zfile.read(cal_files[0]).decode('ascii')\n",
    "            lines = data.splitlines()\n",
    "            _, items, *ignore = lines[0].split(',')\n",
    "            inst, sn, *ignore = items.split()\n",
    "            sn = sn.lstrip('0')\n",
    "            if inst == 'SUNA':\n",
    "                sn = 'NTR-'+sn\n",
    "    if predeployment.get(sn) is None:\n",
    "        predeployment.update({sn: [file]})\n",
    "    else:\n",
    "        predeployment[sn].append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predeployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the serial numbers, link the instrument uids to their pre-deployment files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_dict = {}\n",
    "for uid in sorted(serial_dict.keys()):\n",
    "    sn = serial_dict.get(uid)\n",
    "    if predeployment.get(sn) is not None:\n",
    "        pre_dict.update({uid: sorted(predeployment.get(sn))})\n",
    "    else:\n",
    "        pre_dict.update({uid: None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=======================================================================================================================**\n",
    "Repeat the Pre-deployment process with the post-deployment files. The DCNs are:\n",
    "* ISUS: 3305-00508-XXXXX-A\n",
    "* SUNA: 3305-00527-XXXXX-A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [file for file in os.listdir(doc_directory) if 'A' in file]\n",
    "post_files = []\n",
    "for file in files:\n",
    "    if '508' in file or '527' in file:\n",
    "        post_files.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_paths = []\n",
    "postdeployment = {}\n",
    "for file in post_files:\n",
    "    path = generate_file_path(doc_directory, file, ext=['.zip'])\n",
    "    with ZipFile(path) as zfile:\n",
    "        cal_files = [file for file in zfile.namelist() if file.lower().endswith('.cal')]\n",
    "        if len(cal_files) > 0:\n",
    "            data = zfile.read(cal_files[0]).decode('ascii')\n",
    "            lines = data.splitlines()\n",
    "            _, items, *ignore = lines[0].split(',')\n",
    "            inst, sn, *ignore = items.split()\n",
    "            sn = sn.lstrip('0')\n",
    "            if inst == 'SUNA':\n",
    "                sn = 'NTR-'+sn\n",
    "    if postdeployment.get(sn) is None:\n",
    "        postdeployment.update({sn: [file]})\n",
    "    else:\n",
    "        postdeployment[sn].append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_dict = {}\n",
    "for uid in sorted(serial_dict.keys()):\n",
    "    sn = serial_dict.get(uid)\n",
    "    post_dict.update({uid: postdeployment.get(sn)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=======================================================================================================================**\n",
    "Now, we need to identify the full paths to the relevant files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return the filepaths for the csv files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_paths = {}\n",
    "for uid in sorted(csv_dict.keys()):\n",
    "    paths = []\n",
    "    for file in csv_dict.get(uid):\n",
    "        path = generate_file_path(asset_management_directory, file, ext=['.csv'])\n",
    "        paths.append(path)\n",
    "    csv_paths.update({uid: paths})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return the filepaths for the predeployment files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_paths = {}\n",
    "for uid in sorted(pre_dict.keys()):\n",
    "    paths = []\n",
    "    if pre_dict.get(uid) is not None:\n",
    "        for file in pre_dict.get(uid):\n",
    "            path = generate_file_path(doc_directory, file)\n",
    "            paths.append(path)\n",
    "        pre_paths.update({uid: paths})\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_paths;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return the filepaths for the post-deployment files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_paths = {}\n",
    "for uid in sorted(post_dict.keys()):\n",
    "    paths = []\n",
    "    if post_dict.get(uid) is not None:\n",
    "        for file in post_dict.get(uid):\n",
    "            path = generate_file_path(doc_directory, file)\n",
    "            paths.append(path)\n",
    "        post_paths.update({uid: paths})\n",
    "    else:\n",
    "        post_paths.update({uid: None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_paths;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=======================================================================================================================** Find and return the calibration files which contain vendor supplied calibration information. This is achieved by searching the calibration directories and matching serial numbers to UIDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serial_nums = get_serial_nums(NUTNR, uids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_dict = get_calibration_files(serial_nums, cal_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_paths = {}\n",
    "for uid in sorted(cal_dict.keys()):\n",
    "    paths = []\n",
    "    for file in cal_dict.get(uid):\n",
    "        path = generate_file_path(cal_directory, file, ext=['.zip','.cap', '.txt', '.log'])\n",
    "        paths.append(path)\n",
    "    cal_paths.update({uid: paths})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_paths;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=======================================================================================================================**\n",
    "# Parsing Calibration Coefficients\n",
    "Above, we have worked through identifying and mapping the calibration files, pre-deployment files, and post-deployment files to the individual instruments through their UIDs and serial numbers. The next step is to open the relevant files and parse out the calibration coefficients. This will require writing a parser for the NUTNRs, including sub-functions to handle the different characteristics of the ISUS and SUNA instruments.\n",
    "\n",
    "Start by opening the calibration files and read the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from zipfile import ZipFile\n",
    "\n",
    "class NUTNRCalibration():\n",
    "    # Class that stores calibration values for CTDs.\n",
    "\n",
    "    def __init__(self, uid):\n",
    "        self.serial = None\n",
    "        self.uid = uid\n",
    "        self.coefficients = {\n",
    "            'CC_cal_temp':[],\n",
    "            'CC_di':[],\n",
    "            'CC_eno3':[],\n",
    "            'CC_eswa':[],\n",
    "            'CC_lower_wavelength_limit_for_spectra_fit':'217',\n",
    "            'CC_upper_wavelength_limit_for_spectra_fit':'240',\n",
    "            'CC_wl':[]\n",
    "        }\n",
    "        self.date = []\n",
    "        self.notes = {\n",
    "            'CC_cal_temp':'',\n",
    "            'CC_di':'',\n",
    "            'CC_eno3':'',\n",
    "            'CC_eswa':'',\n",
    "            'CC_lower_wavelength_limit_for_spectra_fit':'217',\n",
    "            'CC_upper_wavelength_limit_for_spectra_fit':'240',\n",
    "            'CC_wl':''\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def uid(self):\n",
    "        return self._uid\n",
    "\n",
    "    @uid.setter\n",
    "    def uid(self, d):\n",
    "        r = re.compile('.{5}-.{6}-.{5}')\n",
    "        if r.match(d) is not None:\n",
    "            self._uid = d\n",
    "        else:\n",
    "            raise Exception(f\"The instrument uid {d} is not a valid uid. Please check.\")\n",
    "            \n",
    "    def load_cal(self, filepath):\n",
    "        \"\"\"\n",
    "        Wrapper function to load all of the calibration coefficients\n",
    "        \n",
    "        Args:\n",
    "            filepath - path to the directory with filename which has the\n",
    "                calibration coefficients to be parsed and loaded\n",
    "        Calls:\n",
    "            open_cal\n",
    "            parse_cal\n",
    "        \"\"\"\n",
    "        \n",
    "        data = self.open_cal(filepath)\n",
    "        \n",
    "        self.parse_cal(data)\n",
    "    \n",
    "    \n",
    "    def open_cal(self, filepath):\n",
    "        \"\"\"\n",
    "        Function that opens and reads in cal file\n",
    "        information for a NUTNR. Zipfiles are acceptable inputs.\n",
    "        \"\"\"\n",
    "        \n",
    "        if filepath.endswith('.zip'):\n",
    "            with ZipFile(filepath) as zfile:\n",
    "                # Check if ISUS or SUNA to get the appropriate name\n",
    "                filename = [name for name in zfile.namelist()\n",
    "                            if name.lower().endswith('.cal') and 'z' not in name.lower()]\n",
    "                \n",
    "                # Get and open the latest calibration file\n",
    "                if len(filename) == 1:\n",
    "                    data = zfile.read(filename[0]).decode('ascii')\n",
    "                    self.source_file(filepath, filename[0])\n",
    "                    \n",
    "                elif len(filename) > 1:\n",
    "                    filename = [max(filename)]\n",
    "                    data = zfile.read(filename[0]).decode('ascii')\n",
    "                    self.source_file(filepath, filename[0])\n",
    "\n",
    "                else:\n",
    "                    FileExistsError(f\"No .cal file found in {filepath}\")\n",
    "                        \n",
    "        elif filepath.lower().endswith('.cal'):\n",
    "            if 'z' not in filepath.lower().split('/')[-1]:\n",
    "                with open(filepath) as file:\n",
    "                    data = file.read()\n",
    "                self.source_file(filepath, file)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        return data\n",
    "            \n",
    "        \n",
    "    def source_file(self, filepath, filename):\n",
    "        \"\"\"\n",
    "        Routine which parses out the source file and filename\n",
    "        where the calibration coefficients are sourced from.\n",
    "        \"\"\"\n",
    "        \n",
    "        if filepath.lower().endswith('.cal'):\n",
    "            dcn = filepath.split('/')[-2]\n",
    "            filename = filepath.split('/')[-1]\n",
    "        else:\n",
    "            dcn = filepath.split('/')[-1]\n",
    "        \n",
    "        self.source = f'Source file: {dcn} > {filename}'\n",
    "        \n",
    "    \n",
    "    def parse_cal(self, data):\n",
    "        \n",
    "        for k,line in enumerate(data.splitlines()):\n",
    "            \n",
    "            if line.startswith('H'):\n",
    "                _, info, *ignore = line.split(',')\n",
    "                \n",
    "                # The first line of the cal file contains the serial number\n",
    "                if k == 0:\n",
    "                    _, sn, *ignore = info.split()\n",
    "                    if 'SUNA' in info:\n",
    "                        self.serial = 'NTR-' + sn\n",
    "                    else:\n",
    "                        self.serial = sn\n",
    "                    \n",
    "                \n",
    "                # File creation time is when the instrument was calibrated.\n",
    "                # May be multiple times for different cal coeffs\n",
    "                if 'file creation time' in info.lower():\n",
    "                    _, _, _, date, time = info.split()\n",
    "                    date_time = pd.to_datetime(date + ' ' + time).strftime('%Y%m%d')\n",
    "                    self.date.append(date_time)\n",
    "                    \n",
    "                # The temperature at which it was calibrated\n",
    "                if 't_cal_swa' in info.lower() or 't_cal' in info.lower():\n",
    "                    _, cal_temp = info.split()\n",
    "                    self.coefficients['CC_cal_temp'] = cal_temp\n",
    "                    \n",
    "            # Now parse the calibration coefficients\n",
    "            if line.startswith('E'):\n",
    "                _, wl, eno3, eswa, _, di = line.split(',')\n",
    "                \n",
    "                self.coefficients['CC_wl'].append(wl)\n",
    "                self.coefficients['CC_di'].append(di)\n",
    "                self.coefficients['CC_eno3'].append(eno3)\n",
    "                self.coefficients['CC_eswa'].append(eswa)\n",
    "                \n",
    "                \n",
    "    def write_csv(self, outpath):\n",
    "        \"\"\"\n",
    "        This function writes the correctly named csv file for the ctd to the\n",
    "        specified directory.\n",
    "\n",
    "        Args:\n",
    "            outpath - directory path of where to write the csv file\n",
    "        Raises:\n",
    "            ValueError - raised if the CTD object's coefficient dictionary\n",
    "                has not been populated\n",
    "        Returns:\n",
    "            self.to_csv - a csv of the calibration coefficients which is\n",
    "                written to the specified directory from the outpath.\n",
    "        \"\"\"\n",
    "\n",
    "        # Run a check that the coefficients have actually been loaded\n",
    "        if len(self.coefficients.values()) <= 2:\n",
    "            raise ValueError('No calibration coefficients have been loaded.')\n",
    "\n",
    "        # Create a dataframe to write to the csv\n",
    "        data = {\n",
    "            'serial': [self.serial]*len(self.coefficients),\n",
    "            'name': list(self.coefficients.keys()),\n",
    "            'value': list(self.coefficients.values())\n",
    "        }\n",
    "        df = pd.DataFrame().from_dict(data)\n",
    "\n",
    "        # Define a function to reformat the notes into an uniform system\n",
    "        def reformat_notes(x):\n",
    "            # First, get rid of \n",
    "            try:\n",
    "                np.isnan(x)\n",
    "                x = ''\n",
    "            except:\n",
    "                x = str(x).replace('[','').replace(']','')\n",
    "            return x\n",
    "        \n",
    "        # Now merge the coefficients dataframe with the notes\n",
    "        if len(self.notes) > 0:\n",
    "            notes = pd.DataFrame().from_dict({\n",
    "                'name':list(self.notes.keys()),\n",
    "                'notes':list(self.notes.values())\n",
    "            })\n",
    "            df = df.merge(notes, how='outer', left_on='name', right_on='name')\n",
    "        else:\n",
    "            df['notes'] = ''\n",
    "            \n",
    "        # Add in the source file\n",
    "        df['notes'].iloc[0] = df['notes'].iloc[0] + ' ' + self.source\n",
    "        \n",
    "        # Sort the data by the coefficient name\n",
    "        df = df.sort_values(by='name')\n",
    "\n",
    "        # Generate the csv name\n",
    "        cal_date = max(self.date)\n",
    "        csv_name = self.uid + '__' + cal_date + '.csv'\n",
    "\n",
    "        # Write the dataframe to a csv file\n",
    "        check = input(f\"Write {csv_name} to {outpath}? [y/n]: \")\n",
    "        # check = 'y'\n",
    "        if check.lower().strip() == 'y':\n",
    "            df.to_csv(outpath+'/'+csv_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=======================================================================================================================**\n",
    "# Source Loading of Calibration Coefficients\n",
    "With a NUTNR Calibration object created, we can now begin parsing the different calibration sources for each NUTNR. We will then compare all of the calibration values from each of the sources, checking for any discrepancies between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, I plan on going through each of the NUTNR UIDs, and parse the data into csvs. For sources which contain multiple sources, I plan on extracting each of the calibrations to a temporary folder using the following structure:\n",
    "\n",
    "    <local working directory>/<temp>/<source>/data/<calibration file>\n",
    "    \n",
    "The separate calibrations will be saved using the standard UFrame naming convention with the following directory structure:\n",
    "\n",
    "    <local working directory>/<temp>/<source>/<calibration csv>\n",
    "    \n",
    "The csvs themselves will also be copied to the temporary folder. This allows for the program to be looking into the same temp directory for every NUTNR check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid = uids[37]\n",
    "print(uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_directory = '/'.join((os.getcwd(),'temp'))\n",
    "# Check if the temp directory exists; if it already does, purge and rewrite\n",
    "if os.path.exists(temp_directory):\n",
    "    shutil.rmtree(temp_directory)\n",
    "    ensure_dir(temp_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the existing csvs from asset management to the temp directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in csv_paths[uid]:\n",
    "    savedir = '/'.join((temp_directory,'csv'))\n",
    "    ensure_dir(savedir)\n",
    "    savepath = '/'.join((savedir, path.split('/')[-1]))\n",
    "    shutil.copyfile(path, savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(temp_directory+'/csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=======================================================================================================================**\n",
    "Load the calibration coefficients from the vendor calibration source files. Start by extracting or copying them to the source data folder in the temporary directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_paths[uid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the calibration zip files to the local temp directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in cal_paths[uid]:\n",
    "    with ZipFile(path) as zfile:\n",
    "        files = [name for name in zfile.namelist() if name.lower().endswith('.cal') and 'Z' not in name]\n",
    "        for file in files:\n",
    "            exdir = path.split('/')[-1].strip('.zip')\n",
    "            expath = '/'.join((temp_directory,'cal','data',exdir))\n",
    "            ensure_dir(expath)\n",
    "            zfile.extract(file,path=expath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the vendor calibration files to csvs following the UFrame convention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savedir = '/'.join((temp_directory,'cal'))\n",
    "ensure_dir(savedir)\n",
    "# Now parse the calibration coefficients\n",
    "for dirpath, dirnames, filenames in os.walk('/'.join((temp_directory,'cal','data'))):\n",
    "    for file in filenames:\n",
    "        filepath = os.path.join(dirpath, file)\n",
    "        # With the filepath for the given calibration retrived, I can now start an instance of the NUTNR Calibration\n",
    "        # object and begin parsing the coefficients\n",
    "        nutnr = NUTNRCalibration(uid)\n",
    "        nutnr.load_cal(filepath)\n",
    "        nutnr.write_csv(savedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=======================================================================================================================**\n",
    "Repeat the above process with the predeployment files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_paths[uid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for path in pre_paths[uid]:\n",
    "        with ZipFile(path) as zfile:\n",
    "            files = [name for name in zfile.namelist() if name.lower().endswith('.cal') and 'Z' not in name]\n",
    "            for file in files:\n",
    "                exdir = path.split('/')[-1].strip('.zip')\n",
    "                expath = '/'.join((temp_directory,'pre','data',exdir))\n",
    "                ensure_dir(expath)\n",
    "                zfile.extract(file,path=expath)\n",
    "    savedir = '/'.join((temp_directory,'pre'))\n",
    "    ensure_dir(savedir)\n",
    "    # Now parse the calibration coefficients\n",
    "    for dirpath, dirnames, filenames in os.walk('/'.join((temp_directory,'pre','data'))):\n",
    "        for file in filenames:\n",
    "            filepath = os.path.join(dirpath, file)\n",
    "            # With the filepath for the given calibration retrived, I can now start an instance of the NUTNR Calibration\n",
    "            # object and begin parsing the coefficients\n",
    "            nutnr = NUTNRCalibration(uid)\n",
    "            nutnr.load_cal(filepath)\n",
    "            nutnr.write_csv(savedir)\n",
    "except KeyError:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=======================================================================================================================**\n",
    "Repeat the above process with the post-deployment files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_paths[uid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if post_paths[uid] is not None:\n",
    "    for path in post_paths[uid]:\n",
    "        with ZipFile(path) as zfile:\n",
    "            files = [name for name in zfile.namelist() if name.lower().endswith('.cal') and 'Z' not in name]\n",
    "            for file in files:\n",
    "                exdir = path.split('/')[-1].strip('.zip')\n",
    "                expath = '/'.join((temp_directory,'post','data',exdir))\n",
    "                ensure_dir(expath)\n",
    "                zfile.extract(file,path=expath)\n",
    "    \n",
    "    savedir = '/'.join((temp_directory,'post'))\n",
    "    ensure_dir(savedir)\n",
    "    # Now parse the calibration coefficients\n",
    "    for dirpath, dirnames, filenames in os.walk('/'.join((temp_directory,'post','data'))):\n",
    "        for file in filenames:\n",
    "            filepath = os.path.join(dirpath, file)\n",
    "            # With the filepath for the given calibration retrived, I can now start an instance of the NUTNR Calibration\n",
    "            # object and begin parsing the coefficients\n",
    "            nutnr = NUTNRCalibration(uid)\n",
    "            nutnr.load_cal(filepath)\n",
    "            nutnr.write_csv(savedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=======================================================================================================================**\n",
    "# Calibration Coefficient Comparison\n",
    "We have now successfully parsed the calibration files from all the possible sources: the vendor calibration files, the pre-deployments files, and the post-deployment files. Furthermore, we have saved csvs in the UFrame format for all of these calibrations. Now, we want to load those csvs into pandas dataframes, which allow for easy element-by-element comparison of calibration coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load the names of the files into a pandas dataframe to compare between the different calibration dates. This will allow for checking of which calibrations should match up to the csv currently contained in asset management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_date(x):\n",
    "    x = str(x)\n",
    "    ind1 = x.index('__')\n",
    "    ind2 = x.index('.')\n",
    "    return x[ind1+2:ind2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV from asset management\n",
    "csv_files = pd.DataFrame(sorted(os.listdir(temp_directory+'/csv')),columns=['csv'])\n",
    "csv_files['cal date'] = csv_files['csv'].apply(lambda x: get_file_date(x))\n",
    "csv_files.set_index('cal date',inplace=True)\n",
    "csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV from vendor calibrations\n",
    "files = sorted([file for file in os.listdir(temp_directory+'/cal') if not os.path.isdir(temp_directory+'/cal/'+file)])\n",
    "cal_files = pd.DataFrame(files,columns=['cal'])\n",
    "cal_files['cal date'] = cal_files['cal'].apply(lambda x: get_file_date(x))\n",
    "cal_files.set_index('cal date',inplace=True)\n",
    "cal_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV from pre-deployment calibrations\n",
    "files = sorted([file for file in os.listdir(temp_directory+'/pre') if not os.path.isdir(temp_directory+'/pre/'+file)])\n",
    "pre_files = pd.DataFrame(files,columns=['pre'])\n",
    "pre_files['cal date'] = pre_files['pre'].apply(lambda x: get_file_date(x))\n",
    "pre_files.set_index('cal date',inplace=True)\n",
    "pre_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV from post-deployment calibrations\n",
    "files = sorted([file for file in os.listdir(temp_directory+'/post') if not os.path.isdir(temp_directory+'/post/'+file)])\n",
    "post_files = pd.DataFrame(files,columns=['post'])\n",
    "post_files['cal date'] = post_files['post'].apply(lambda x: get_file_date(x))\n",
    "post_files.set_index('cal date',inplace=True)\n",
    "post_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the different source file dataframes together for easy visual comparison\n",
    "df_files = csv_files.join(cal_files,how='outer')\n",
    "df_files = df_files.join(pre_files,how='outer')\n",
    "df_files = df_files.join(post_files,how='outer')\n",
    "df_files = df_files.fillna(value='-999')\n",
    "df_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the above dataframe to assess which files correspond to each other. If any of the csv files need to be renamed, now is the time to go ahead and do so. This will allow for direct comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = '/'.join((os.getcwd(),'temp','csv','CGINS-NUTNRB-01107__20171128.csv'))\n",
    "dst = '/'.join((os.getcwd(),'temp','csv','CGINS-NUTNRB-01107__20171129.csv'))\n",
    "shutil.move(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV from asset management\n",
    "csv_files = pd.DataFrame(sorted(os.listdir(temp_directory+'/csv')),columns=['csv'])\n",
    "csv_files['cal date'] = csv_files['csv'].apply(lambda x: get_file_date(x))\n",
    "csv_files.set_index('cal date',inplace=True)\n",
    "csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the different source file dataframes together for easy visual comparison\n",
    "df_files = csv_files.join(cal_files,how='outer')\n",
    "df_files = df_files.join(pre_files,how='outer')\n",
    "df_files = df_files.join(post_files,how='outer')\n",
    "df_files = df_files.fillna(value='-999')\n",
    "df_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have renamed any csv files to their likely calibration source. Our next step is to do the actual coefficient comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_directory = '/'.join((temp_directory,'csv'))\n",
    "fname = 'CGINS-NUTNRB-01107__20181011.csv'\n",
    "CSV = pd.read_csv(load_directory+'/'+fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_directory = '/'.join((temp_directory,'pre'))\n",
    "PRE = pd.read_csv(load_directory+'/'+fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_directory = '/'.join((temp_directory,'post'))\n",
    "POST = pd.read_csv(load_directory+'/'+fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_directory = '/'.join((temp_directory,'cal'))\n",
    "CAL = pd.read_csv(load_directory+'/'+fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_arrays(array):\n",
    "    # First, need to strip extraneous characters from the array\n",
    "    array = array.replace(\"'\",\"\").replace('[','').replace(']','')\n",
    "    # Next, split the array into a list\n",
    "    array = array.split(',')\n",
    "    # Now, need to eliminate any white space surrounding the individual coeffs\n",
    "    array = [num.strip() for num in array]\n",
    "    # Next, float the nums\n",
    "    array = [float(num) for num in array]\n",
    "    # Check if the array is len == 1; if so, can just return the number\n",
    "    if len(array) == 1:\n",
    "        array = array[0]\n",
    "    # Now we are done\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV['value'] = CSV['value'].apply(lambda x: reformat_arrays(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE['value'] = PRE['value'].apply(lambda x: reformat_arrays(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAL['value'] = CAL['value'].apply(lambda x: reformat_arrays(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POST['value'] = POST['value'].apply(lambda x: reformat_arrays(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(PRE['notes'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.equal(CSV['value'],PRE['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isclose(CSV['value'].iloc[1],PRE['value'].iloc[1],rtol=1e-8,atol=1e-11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isclose(PRE['value'].iloc[1],CSV['value'].iloc[1],rtol=1e-8,atol=1e-11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_cal_coeffs(coeffs_dict):\n",
    "    \n",
    "    # Part 1: coeff by coeff comparison between each source of coefficients\n",
    "    keys = list(coeffs_dict.keys())\n",
    "    comparison = {}\n",
    "    for i in range(len(keys)):\n",
    "        names = (keys[i], keys[i - (len(keys)-1)])\n",
    "        check = len(coeffs_dict.get(keys[i])['value']) == len(coeffs_dict.get(keys[i - (len(keys)-1)])['value'])\n",
    "        if check:\n",
    "            compare = np.isclose(coeffs_dict.get(keys[i])['value'], coeffs_dict.get(keys[i - (len(keys)-1)])['value'])\n",
    "            comparison.update({names:compare})\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    # Part 2: now do a logical_and comparison between the results from part 1\n",
    "    keys = list(comparison.keys())\n",
    "    i = 0\n",
    "    mask = comparison.get(keys[i])\n",
    "    while i < len(keys)-1:\n",
    "        i = i + 1\n",
    "        mask = np.logical_and(mask, comparison.get(keys[i]))\n",
    "        print(i)\n",
    "       \n",
    "    return mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {}\n",
    "for cal_date in df_files.index:\n",
    "    # Part 1, load all of the csv files\n",
    "    coeffs_dict = {}\n",
    "    for source,fname in df_files.loc[cal_date].items():\n",
    "        if fname != '-999':\n",
    "            load_directory = '/'.join((os.getcwd(),'temp',source,fname))\n",
    "            df_coeffs = pd.read_csv(load_directory)\n",
    "            for i in list(set(df_coeffs['serial'])):\n",
    "                print(source + '-' + fname + ': ' + str(i))\n",
    "            df_coeffs.set_index(keys='name',inplace=True)\n",
    "            df_coeffs.sort_index(inplace=True)\n",
    "            coeffs_dict.update({source:df_coeffs})\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    # Part 2, now check the calibration coefficients\n",
    "    mask = check_cal_coeffs(coeffs_dict)\n",
    "    \n",
    "    # Part 3: get the calibration coefficients are wrong\n",
    "    # and show them\n",
    "    fname = df_files.loc[cal_date]['csv']\n",
    "    if fname == '-999':\n",
    "        incorrect = 'No csv file.'\n",
    "    else:\n",
    "        incorrect = coeffs_dict['csv'][mask == False]\n",
    "    result.update({fname:incorrect})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
